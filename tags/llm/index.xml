<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on Jamie&#39;s Blog</title>
    <link>http://akjamie.github.io/tags/llm/</link>
    <description>Recent content in LLM on Jamie&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Sep 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://akjamie.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Learning Notes: Fine-Tuning Transformer Models</title>
      <link>http://akjamie.github.io/post/2025-09-02-learning-notes-transformer-finetune/</link>
      <pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://akjamie.github.io/post/2025-09-02-learning-notes-transformer-finetune/</guid>
      <description>Fine-Tuning Transformer Models: A Beginner&amp;rsquo;s Guide Fine-tuning is the process of adapting a pre-trained transformer model (like GPT, BERT, etc.) to a specific task or dataset. This blog will help you understand the basics, workflow, and best practices for fine-tuning.&#xA;What is Fine-Tuning? Pre-trained models learn general language patterns from massive datasets. Fine-tuning adapts these models to your specific task (e.g., sentiment analysis, Q&amp;amp;A, summarization) using a smaller, task-specific dataset. flowchart LR&#xD;A[Pre-trained Model] --&gt; B[Fine-tuning Process]&#xD;C[Task-specific Dataset] --&gt; B&#xD;B --&gt; D[Fine-tuned Model]&#xD;Why Fine-Tune?</description>
    </item>
    <item>
      <title>Understanding Transformer Architecture: The Foundation of Modern AI</title>
      <link>http://akjamie.github.io/post/2025-08-31-learning-notes-transformer/</link>
      <pubDate>Sun, 31 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://akjamie.github.io/post/2025-08-31-learning-notes-transformer/</guid>
      <description>Introduction to Transformer Architecture In the rapidly evolving world of artificial intelligence, few innovations have been as transformative as the Transformer architecture. Introduced in the seminal 2017 paper &amp;ldquo;Attention is All You Need&amp;rdquo; by Vaswani et al., Transformers have become the backbone of virtually all state-of-the-art language models, including GPT-4, ChatGPT, and Google&amp;rsquo;s Bard.&#xA;But what exactly is a Transformer, and why has it revolutionized natural language processing? In this comprehensive guide, we&amp;rsquo;ll break down the Transformer architecture from the ground up, using clear explanations and visual diagrams to help you understand how these powerful models work.</description>
    </item>
    <item>
      <title>Run LLM on Intel Iris Xe GPU Using IEPX-LLM &#43; Ollama</title>
      <link>http://akjamie.github.io/post/2024-06-15-run-llm-on-intel-igpu/</link>
      <pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://akjamie.github.io/post/2024-06-15-run-llm-on-intel-igpu/</guid>
      <description>In the realm of AI and machine learning, optimizing performance often hinges on utilizing GPU acceleration. However, Intel GPUs traditionally have not supported running AI workloads directly with popular frameworks like TensorFlow or PyTorch [5]. To address this, developers can turn to IEPX-LLM, a specialized library tailored for Intel&amp;rsquo;s XPU architecture.&#xA;Steps to Run LLM on Intel GPU with IEPX-LLM + Ollama Locally Install Prerequisites (Optional) Update GPU Driver Tips :information_source: It is recommended to update your GPU driver, if you have driver version lower than 31.</description>
    </item>
  </channel>
</rss>
