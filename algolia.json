[{"author":null,"categories":["AI","Deep Learning"],"content":"Fine-Tuning Transformer Models: A Beginner\u0026amp;rsquo;s Guide Fine-tuning is the process of adapting a pre-trained transformer model (like GPT, BERT, etc.) to a specific task or dataset. This blog will help you understand the basics, workflow, and best practices for fine-tuning.\nWhat is Fine-Tuning? Pre-trained models learn general language patterns from massive datasets. Fine-tuning adapts these models to your specific task (e.g., sentiment analysis, Q\u0026amp;amp;A, summarization) using a smaller, task-specific dataset. flowchart LR\rA[Pre-trained Model] --\u0026gt; B[Fine-tuning Process]\rC[Task-specific Dataset] --\u0026gt; B\rB --\u0026gt; D[Fine-tuned Model]\rWhy Fine-Tune? Saves time and resources (no need to train from scratch) Leverages powerful language understanding Achieves state-of-the-art results on custom tasks Typical Fine-Tuning Workflow Choose a Pre-trained Model Popular choices: GPT, BERT, RoBERTa, T5, etc. Prepare Your Dataset Format data for your task (classification, generation, etc.) Clean and split …","date":1756771200,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1000,"html":"A beginner-friendly guide to fine-tuning transformer models, including practical steps, tips, and common pitfalls.","keywords":null,"kind":"page","lang":"en","lastmod":1756771200,"objectID":"95abde8174c4db897b8ab286591304da","permalink":"http://akjamie.github.io/post/2025-09-02-learning-notes-transformer-finetune/","publishdate":"2025-09-02T00:00:00Z","readingtime":5,"relpermalink":"/post/2025-09-02-learning-notes-transformer-finetune/","section":"post","tags":["LLM","Transformer","Fine-tune","AI"],"title":"Learning Notes: Fine-Tuning Transformer Models","type":"post","url":"/post/2025-09-02-learning-notes-transformer-finetune/","weight":0,"wordcount":941},{"author":null,"categories":["AI"],"content":"Introduction to Transformer Architecture In the rapidly evolving world of artificial intelligence, few innovations have been as transformative as the Transformer architecture. Introduced in the seminal 2017 paper \u0026amp;ldquo;Attention is All You Need\u0026amp;rdquo; by Vaswani et al., Transformers have become the backbone of virtually all state-of-the-art language models, including GPT-4, ChatGPT, and Google\u0026amp;rsquo;s Bard.\nBut what exactly is a Transformer, and why has it revolutionized natural language processing? In this comprehensive guide, we\u0026amp;rsquo;ll break down the Transformer architecture from the ground up, using clear explanations and visual diagrams to help you understand how these powerful models work.\nWhether you\u0026amp;rsquo;re a beginner just starting your AI journey or a practitioner looking to solidify your understanding, this guide will walk you through the core concepts that power today\u0026amp;rsquo;s most advanced language models.\nThe Problem with the Past: RNNs and Feedforward Networks Before …","date":1756598400,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":3000,"html":"Learn how Transformer architecture works with clear explanations and visual diagrams. Understand the core concepts behind modern AI models like GPT-4 and GPT-5.","keywords":null,"kind":"page","lang":"en","lastmod":1756598400,"objectID":"1e6c6cc52bdd9b607cd86bcbdc37563b","permalink":"http://akjamie.github.io/post/2025-08-31-learning-notes-transformer/","publishdate":"2025-08-31T00:00:00Z","readingtime":14,"relpermalink":"/post/2025-08-31-learning-notes-transformer/","section":"post","tags":["LLM","Transformer","Machine Learning","Deep Learning","Attention Mechanism"],"title":"Understanding Transformer Architecture: The Foundation of Modern AI","type":"post","url":"/post/2025-08-31-learning-notes-transformer/","weight":0,"wordcount":2980},{"author":null,"categories":["AI"],"content":"In the realm of AI and machine learning, optimizing performance often hinges on utilizing GPU acceleration. However, Intel GPUs traditionally have not supported running AI workloads directly with popular frameworks like TensorFlow or PyTorch [5]. To address this, developers can turn to IEPX-LLM, a specialized library tailored for Intel\u0026amp;rsquo;s XPU architecture.\nSteps to Run LLM on Intel GPU with IEPX-LLM + Ollama Locally Install Prerequisites (Optional) Update GPU Driver Tips :information_source: It is recommended to update your GPU driver, if you have driver version lower than 31.0.101.5122. Refer to here for more information.\nDownload and install the latest GPU driver from the official Intel download page. A system reboot is necessary to apply the changes after the installation is complete.\nThe process could take around 10 minutes. After reboot, check for the Intel Arc Control application to verify the driver has been installed correctly. If the installation was successful, you …","date":1718409600,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":700,"html":"For windows demostration only.","keywords":null,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"8911984bc67c9dd65ddd753b16c396e0","permalink":"http://akjamie.github.io/post/2024-06-15-run-llm-on-intel-igpu/","publishdate":"2024-06-15T00:00:00Z","readingtime":4,"relpermalink":"/post/2024-06-15-run-llm-on-intel-igpu/","section":"post","tags":["LLM","iGPU"],"title":"Run LLM on Intel Iris Xe GPU Using IEPX-LLM + Ollama","type":"post","url":"/post/2024-06-15-run-llm-on-intel-igpu/","weight":0,"wordcount":658},{"author":null,"categories":["Cloud"],"content":" Complains about the CKA exam:\nThe CKA exam environment was changed from Terminal to Remote Desktop(VNC) from Last Jun, this is an absolutely shit\ndecision. The huge lagging causes that large amount of candidate\u0026amp;rsquo;s time-wasting on always-waiting, and also Mouse is nearly useless - very difficult to locate what you want on Firefox browser, and eventually it highlights the good user experience on the k8s built-in documentation - Kubectl explain, how ridiculous it is !!!\nImportant commands Key file path Path Usage Remark /etc/systemd/system/kubelet.service.d \\n /lib/systemd/system/kubelet.service kubelet service /lib/systemd/system/containerd.service containerd service /var/lib/kubelet ephemeral storage path it will cause cluster HasDiskPressure if 85% disk of it is being used and cannot being re-cycled /var/lib/containerd Image Fs it will cause cluster HasDiskPressure if 85% disk of it is being used and cannot being re-cycled /etc/kubernetes K8s cluster config, certs, static pods …","date":1685318400,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":700,"html":"","keywords":null,"kind":"page","lang":"en","lastmod":1685318400,"objectID":"9b57077a61780adebc3e50dc9a0fda24","permalink":"http://akjamie.github.io/post/2023-05-29-k8s-cka/","publishdate":"2023-05-29T00:00:00Z","readingtime":4,"relpermalink":"/post/2023-05-29-k8s-cka/","section":"post","tags":["Kubernetes"],"title":"CKA \u0026 important operation commands","type":"post","url":"/post/2023-05-29-k8s-cka/","weight":0,"wordcount":690},{"author":null,"categories":["Cloud"],"content":"A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported. Compared to hostPath volumes, local volumes are used in a durable and portable manner without manually scheduling pods to nodes. The system is aware of the volume\u0026amp;rsquo;s node constraints by looking at the node affinity on the PersistentVolume.\nLocal Storage vs Dynamic/remove storage Storage type Storage class/provisioner Pros Cons HostPath Not applicable Not recommended to use. - HostPaths can expose privileged system credentials (such as for the Kubelet) or privileged APIs (such as container runtime socket), which can be used for container escape or to attack other parts of the cluster. - Pods with identical configuration (such as created from a PodTemplate) may behave differently on different nodes due to different files on the nodes. - The files or directories created on the …","date":1680912000,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1700,"html":"A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported.","keywords":null,"kind":"page","lang":"en","lastmod":1680912000,"objectID":"2581cc6ed61f532e7f782d80ccfdafc8","permalink":"http://akjamie.github.io/post/2023-04-08-k8s-pv-pvc-access-mode/","publishdate":"2023-04-08T00:00:00Z","readingtime":8,"relpermalink":"/post/2023-04-08-k8s-pv-pvc-access-mode/","section":"post","tags":["Kubernetes"],"title":"Explore Kubernetes Local PV \u0026 PVC access mode","type":"post","url":"/post/2023-04-08-k8s-pv-pvc-access-mode/","weight":0,"wordcount":1643},{"author":null,"categories":["Cloud"],"content":"Challenge Nearly all requests to Vault must be accompanied by an authentication token. This includes all API requests, as well as via the Vault CLI and other libraries. If you can securely get the first secret from an originator to a consumer, all subsequent secrets transmitted between this originator and consumer can be authenticated with the trust established by the successful distribution and user of that first secret.\nThe applications running in a Kubernetes environment is no exception. Luckily, Vault provides Kubernetes auth method to authenticate the clients using a Kubernetes Service Account Token.\nHowever, Client is still responsible for managing the lifecycle of its Vault Tokens as illustrated on below diagram of Vault Workflow on kubernetes. Major processes: - One-off setup/infrequent action for initial configuration \u0026amp;amp; standard policy maintenance, e.g. enable auth method, create roles,policies. - Pod deployed with JWT token(service account token) injected. - API calls to …","date":1679788800,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":800,"html":"Nearly all requests to Vault must be accompanied by an authentication token. This includes all API requests, as well as via the Vault CLI and other libraries, therefore application running in kubernetes is no exception. Luckily, Vault provides Kubernetes auth method to authenticate the clients using a Kubernetes Service Account Token, and Vault Agent which could be leveraged to automatically inject the secrets from vault into kubernetes pods through init container pattern.","keywords":null,"kind":"page","lang":"en","lastmod":1679788800,"objectID":"f7719d87c3a916b00ae4840a78d75923","permalink":"http://akjamie.github.io/post/2023-03-26-vault-agent-with-k8s/","publishdate":"2023-03-26T00:00:00Z","readingtime":4,"relpermalink":"/post/2023-03-26-vault-agent-with-k8s/","section":"post","tags":["Vault","Kubernetes"],"title":"Vault Agent with Kubernetes","type":"post","url":"/post/2023-03-26-vault-agent-with-k8s/","weight":0,"wordcount":711},{"author":null,"categories":["Cloud"],"content":"What is Vault? Vault introduction HashiCorp Vault is an identity-based secrets and encryption management system. A secret is anything that you want to tightly\ncontrol access to, such as API encryption keys, passwords, and certificates. Vault provides encryption services that are gated by authentication and authorization methods.Using Vault’s UI, CLI, or HTTP API, access to secrets and other sensitive data can be securely stored and managed, tightly controlled (restricted), and auditable.\nA modern system requires access to a multitude of secrets, including database credentials, API keys for external services, credentials for service-oriented architecture communication, etc. It can be difficult to understand who is accessing which secrets, especially since this can be platform-specific. Adding on key rolling, secure storage, and detailed audit logs is almost impossible without a custom solution. This is where Vault steps in.\nKey features of Vault Secure Secret Storage - Arbitrary …","date":1679702400,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1600,"html":"To setup a HA Vault Cluster in Kubernetes","keywords":null,"kind":"page","lang":"en","lastmod":1679702400,"objectID":"aab31349cf68ab8cd7c2f90c685c16b6","permalink":"http://akjamie.github.io/post/2023-03-25-vault-on-k8s/","publishdate":"2023-03-25T00:00:00Z","readingtime":8,"relpermalink":"/post/2023-03-25-vault-on-k8s/","section":"post","tags":["Vault","Kubernetes"],"title":"Vault on Kubernetes","type":"post","url":"/post/2023-03-25-vault-on-k8s/","weight":0,"wordcount":1524},{"author":null,"categories":["NoSQL"],"content":"What\u0026amp;rsquo;s Redis Cluster Redis Cluster is a distributed implementation of Redis with the following goals in order of importance in the design:\nHigh performance and linear scalability up to 1000 nodes. There are no proxies, asynchronous replication is used, and no merge operations are performed on values. 在多达 1000 个节点的时候仍能保持高性能及线性的可扩展性。没有代理，使用异步复制，并且不对值执行合并操作 Acceptable degree of write safety: the system tries (in a best-effort way) to retain all the writes originating from clients connected with the majority of the master nodes. Usually there are small windows where acknowledged writes can be lost. Windows to lose acknowledged writes are larger when clients are in a minority partition. 可接受的写入安全：与多数派节点相连的客户端所做的写入操作，系统尝试全部都保存下来（以最大努力的方式）。不过极小概率下容忍小部分写入会丢失 Availability: Redis Cluster is able to survive partitions where the majority of the master nodes are reachable and there is at least one reachable replica for every master node that is no longer reachable. Moreover using replicas …","date":1657407810,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1500,"html":"Redis 集群","keywords":null,"kind":"page","lang":"en","lastmod":1657407810,"objectID":"57360c3a8a1b44aeccfd78f0fcfbbbe7","permalink":"http://akjamie.github.io/post/2022-07-09-redis-cluster-01/","publishdate":"2022-07-09T23:03:30Z","readingtime":3,"relpermalink":"/post/2022-07-09-redis-cluster-01/","section":"post","tags":["Redis","NoSQL"],"title":"Redis advance III","type":"post","url":"/post/2022-07-09-redis-cluster-01/","weight":0,"wordcount":1454},{"author":null,"categories":["Microservice"],"content":"Background 最近在做一个类似于快速竞价的功能的技术设计，即在很短的时间内通过一个集中的平台拿到各家的报价，类似于支付宝上的那个车险报价，发送完汽车信息，得到各家保险公司的报价回复，然后可以选择某家性价比高的价格购买汽车保险。在设计的过程很快就想到了用websocket+stomp这种长链接和event-streaming 推送的方式，做了一些技术研究和验证，因此便记录这些零碎的知识便于以后查阅。\nReffering articles 1.偶然找到了下面这篇文章，写的挺详细的便引用过来，弥补了spring官方文档不够详细的不足。\nhttps://medium.com/swlh/websockets-with-spring-part-1-http-and-websocket-36c69df1c2ee\nhttps://medium.com/swlh/websockets-with-spring-part-2-websocket-with-sockjs-fallback-1903cf8fe480\nhttps://medium.com/swlh/websockets-with-spring-part-3-stomp-over-websocket-3dab4a21f397\n","date":1656892800,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":300,"html":"记录websocket集成的杂事","keywords":null,"kind":"page","lang":"en","lastmod":1656892800,"objectID":"ec493e039d35b0eef439cb4e016fc0e0","permalink":"http://akjamie.github.io/post/2022-07-04-spring-websocket-stomp/","publishdate":"2022-07-04T00:00:00Z","readingtime":1,"relpermalink":"/post/2022-07-04-spring-websocket-stomp/","section":"post","tags":["Websocket","Spring-stomp","K8S"],"title":"Websocket集成","type":"post","url":"/post/2022-07-04-spring-websocket-stomp/","weight":0,"wordcount":245},{"author":null,"categories":["NoSQL"],"content":"主从复制 基本概念 主机数据更新后根据配置和策略，自动同步到备份机的master/slave机制，其中master以写为主，而slave主要以读为主。 该机制使\n1》读写分离，性能获得提升\n2》容灾快速恢复\n环境搭建 快速demo搭建一主两从的主从复制集群，准备3份配置文件：\nredis-6379.conf redis-6380.conf redis-6381.conf # ----------------------------------------------------------------------- # redis-6379.conf ==\u0026amp;gt; master node # include original redis config - full version include /Users/jamie/Documents/work-benches/redis/redis-6.0.6/redis.conf #pidfile path pidfile /var/run/redis-6379.pid #redis listening port port 6379 #rdb filename dbfilename dump-6379.rdb #appendonly filename appendfilename appendonly-6379.aof daemonize yes # ----------------------------------------------------------------------- # redis-6380.conf ==\u0026amp;gt; slave node # include original redis config - full version include /Users/jamie/Documents/work-benches/redis/redis-6.0.6/redis.conf #pidfile path pidfile /var/run/redis-6380.pid #redis listening port port 6380 #rdb filename dbfilename dump-6380.rdb #appendonly filename appendfilename …","date":1656374400,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1500,"html":"Redis主从复制","keywords":null,"kind":"page","lang":"en","lastmod":1656374400,"objectID":"c3274eef906dd927be1de9ea233d52ba","permalink":"http://akjamie.github.io/post/2022-06-28-redis-master-slave-replication/","publishdate":"2022-06-28T00:00:00Z","readingtime":3,"relpermalink":"/post/2022-06-28-redis-master-slave-replication/","section":"post","tags":["Redis","NoSQL","K8S"],"title":"Redis advance II","type":"post","url":"/post/2022-06-28-redis-master-slave-replication/","weight":0,"wordcount":1401},{"author":null,"categories":["NoSQL"],"content":"Redis 事务 事务概念和基本操作 其为命令提供一个单独的隔离环境，事务中所有的命令都会序列化，按顺序地执行，执行过程中不会被其他客户端发送过来的命令请求打断。 与dbms事物不同，redis事务的作用就是串联多个命令防止被其他命令插队\ndiscard sample 127.0.0.1:6379[1]\u0026amp;gt; keys * (empty array) 127.0.0.1:6379[1]\u0026amp;gt; multi OK 127.0.0.1:6379[1]\u0026amp;gt; set key-1 test-1 QUEUED 127.0.0.1:6379[1]\u0026amp;gt; mset key-2 test-2 key-3 test-3 QUEUED 127.0.0.1:6379[1]\u0026amp;gt; discard OK 127.0.0.1:6379[1]\u0026amp;gt; keys * (empty array) exec succ 127.0.0.1:6379[1]\u0026amp;gt; keys * (empty array) 127.0.0.1:6379[1]\u0026amp;gt; multi OK 127.0.0.1:6379[1]\u0026amp;gt; mset key-2 test-2 key-3 test-3 QUEUED 127.0.0.1:6379[1]\u0026amp;gt; exec 1) OK 127.0.0.1:6379[1]\u0026amp;gt; keys * 1) \u0026amp;#34;key-2\u0026amp;#34; 2) \u0026amp;#34;key-3\u0026amp;#34; 127.0.0.1:6379[1]\u0026amp;gt; error in middle of exec 127.0.0.1:6379[1]\u0026amp;gt; multi OK 127.0.0.1:6379[1]\u0026amp;gt; hget hkey-1 color QUEUED 127.0.0.1:6379[1]\u0026amp;gt; incr key-2 QUEUED 127.0.0.1:6379[1]\u0026amp;gt; hset hkey-1 color blue QUEUED 127.0.0.1:6379[1]\u0026amp;gt; exec 1) (nil) 2) (error) ERR value is not an integer or out of range 3) (integer) 1 127.0.0.1:6379[1]\u0026amp;gt; hget hkey-1 color …","date":1656288000,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1600,"html":"Redis事务和持久化","keywords":null,"kind":"page","lang":"en","lastmod":1656288000,"objectID":"627e17847d3ac2626ecc169cf56b8857","permalink":"http://akjamie.github.io/post/2022-06-27-redis-basic-2/","publishdate":"2022-06-27T00:00:00Z","readingtime":4,"relpermalink":"/post/2022-06-27-redis-basic-2/","section":"post","tags":["Redis","NoSQL"],"title":"Redis foundation - III","type":"post","url":"/post/2022-06-27-redis-basic-2/","weight":0,"wordcount":1521},{"author":null,"categories":["NoSQL"],"content":"bitmap 在前面的基础回顾中提到了bitmap这种数据结构，主要用来应对string的位操作，简单高效\n适用场景：单状态统计，如每天系统用户登录情况统计，github commit.\n其常用命令有 setbit, getbit, bitcount, bitop.\n127.0.0.1:6379\u0026amp;gt; setbit test:bitmaps:2 0 1 (integer) 0 127.0.0.1:6379\u0026amp;gt; bitcount test:bitmaps:2 (integer) 1 127.0.0.1:6379\u0026amp;gt; setbit test:bitmaps:2 0 2 --\u0026amp;gt; the value on each bit is only 0 and 1 (error) ERR bit is not an integer or out of range 127.0.0.1:6379\u0026amp;gt; setbit test:bitmaps:2 1 1 (integer) 0 127.0.0.1:6379\u0026amp;gt; setbit test:bitmaps:2 3 1 (integer) 0 127.0.0.1:6379\u0026amp;gt; bitcount test:bitmaps:2 --\u0026amp;gt; 1101 (integer) 3 127.0.0.1:6379\u0026amp;gt; bitop not notValue test:bitmaps:2 --\u0026amp;gt; 0010 (integer) 1 127.0.0.1:6379\u0026amp;gt; getbit notValue 0 (integer) 0 127.0.0.1:6379\u0026amp;gt; getbit notValue 1 (integer) 0 127.0.0.1:6379\u0026amp;gt; getbit notValue 2 (integer) 1 业务用例 整型数据排序/去重 (这个其实不算做bitmap的一个用例场景)\n思路分析:\nbitmap是用每一个位来记录事物的状态，最大长度512Mb = 512 * 1024 * 1024 * 8 = 2^32, 即 8bit = 1byte， offset maximum = 2^32 -1 = 4294967295,即最大的下标为4294967295 将整型数据直接offset设置到bitmap存储结构中， …","date":1655942400,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1900,"html":"Redis进阶知识总结","keywords":null,"kind":"page","lang":"en","lastmod":1655942400,"objectID":"a58bda2dc2e983ffccb2106a46043271","permalink":"http://akjamie.github.io/post/2022-06-23-redis-advance/","publishdate":"2022-06-23T00:00:00Z","readingtime":4,"relpermalink":"/post/2022-06-23-redis-advance/","section":"post","tags":["Redis","NoSQL"],"title":"Redis advance I","type":"post","url":"/post/2022-06-23-redis-advance/","weight":0,"wordcount":1852},{"author":null,"categories":["NoSQL"],"content":"Redis基础 Common知识 Redis 有16个数据库，一般都使用0号库，使用select dbid 来切换db。\nRedis是单线程 + 多路io服用技术\nRedis vs memcache\nRedis -\u0026amp;gt; 单线程 + 多路io复用 + 灵活支持多种数据类型 Memcached -\u0026amp;gt; 多线程+锁 + 单一数据类型 Key 操作\nkeys * - list keys expire key duration - set expire duration for key exists key - verify if key exists del key - delete key-value pair via key unlink key - delete data in non-blocking way ttl key - check remaining expiry duration 127.0.0.1:6379\u0026amp;gt; dbsize (integer) 7 127.0.0.1:6379\u0026amp;gt; type test-set-1 set 127.0.0.1:6379\u0026amp;gt; type test-zset-1 zset 127.0.0.1:6379\u0026amp;gt; exists test-set-1 (integer) 1 127.0.0.1:6379\u0026amp;gt; set test-string-1 \u0026amp;#34;test\u0026amp;#34; OK 127.0.0.1:6379\u0026amp;gt; expire test-string-1 100 (integer) 1 127.0.0.1:6379\u0026amp;gt; ttl test-string-1 (integer) 95 重要命令 select - select db dbsize - check total key size in current db flushdb - cleanup current db flushall - cleanup all dbs 5大数据结构 String 是redis最基本的类型，redis中字符串key和value最大可以存放512m setex key ttl value 存储方式为预分配冗余空间来减少内存的频繁分配 List - 双向链表，一键多值 quickList, 元素少的时候是连续空间存储 - …","date":1653868800,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1700,"html":"Redis基础知识回顾","keywords":null,"kind":"page","lang":"en","lastmod":1653868800,"objectID":"4d03a5849228f6e62482d3254299bef5","permalink":"http://akjamie.github.io/post/2022-05-31-redis-basic-1/","publishdate":"2022-05-30T00:00:00Z","readingtime":4,"relpermalink":"/post/2022-05-31-redis-basic-1/","section":"post","tags":["Redis","NoSQL"],"title":"Redis foundation - II","type":"post","url":"/post/2022-05-31-redis-basic-1/","weight":0,"wordcount":1649},{"author":null,"categories":["Cloud"],"content":"Install dashboard To run the following command to deploy dashboard\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml once done, to check the resource status under the namespace of kubernetes-dashboard, if all the resources are running, that means dashboard is deployed successfully. ps:\nOn above screenshot, you can see the EXTERNAL-IP for kubernetes-dashboard service, the reason is i edit the service change the type from ClusterIp to LoadBalancer, and the loadbalancer is backed by Metallb. Brower(Chrome) will block the dasboard access though the https://192.168.58.156 since the tls cert is generated by kubernetes during installation, it can not be trusted by CA, this issue will be resolved in later step. Generate TLS Cert Suppose you have already a domain name, or buy a domain name from Alicloud, AWS, or godady,etc. Here i will use Alicould for instance, my Domain name is it-meta.space.\nCreate free TSL cert the menu naviagation is …","date":1649721600,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":900,"html":"Deploy K8S dashboard with TLS enabled","keywords":null,"kind":"page","lang":"en","lastmod":1649721600,"objectID":"94a7ff40d3c2d45b2e0f360946bbcdbb","permalink":"http://akjamie.github.io/post/2022-04-12-kubernetes-dashboard/","publishdate":"2022-04-12T00:00:00Z","readingtime":4,"relpermalink":"/post/2022-04-12-kubernetes-dashboard/","section":"post","tags":["Kubernetes","DevOps"],"title":"Kubernetes dashboard deployment","type":"post","url":"/post/2022-04-12-kubernetes-dashboard/","weight":0,"wordcount":807},{"author":null,"categories":["Cloud"],"content":"I\u0026amp;rsquo;m on the path of AWS Professinal Certified Architect exam preparation, want to document some key points in the demos for later review to enhance the understanding of knowledge. In this demo, i\u0026amp;rsquo;m going to test the AWS auto scaling and utilize the EC2 network knowledges.\nObjective To build a health checking springboot application and package as docker image hosted on AWS ECR. Setup basic network infrastructure, VPC, Subnet, Internet Gateway, Route table,Security group, etc. Create Auto Scaling Group using prepred Lanuch Template, and expose the access entry through Application Loader Balancer. To use Apache HTTP server benchmarking tool(ab) to trigger billions of request to test the instance scale out/in. Architecture diagram high level architecture diagram is for easier understanding of this demo. The diagram illustrates an application load balancers to route traffic to EC2 instances hosted in subnet-a(ap-northeast-1c) and subnet-c(ap-northeast-1d), these EC2 instances …","date":1628467200,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":900,"html":"To mark down some key steps for later review","keywords":null,"kind":"page","lang":"en","lastmod":1628467200,"objectID":"9f5d41a6aef178dbadd399e3a59c948d","permalink":"http://akjamie.github.io/post/2021-08-09-aws-demo-01-autoscale-test/","publishdate":"2021-08-09T00:00:00Z","readingtime":5,"relpermalink":"/post/2021-08-09-aws-demo-01-autoscale-test/","section":"post","tags":["AWS","Cloud"],"title":"Demo - AWS EC2 instance auto scaling","type":"post","url":"/post/2021-08-09-aws-demo-01-autoscale-test/","weight":0,"wordcount":886},{"author":null,"categories":["Microservice"],"content":"Summary To mark down a batch data sync accross system solution, tested daily sync data volume is around 10w.\nIn this solution, adopted the message driven batch processing rather than traditional fixed time schedule batch.\nAlso applied the microservice design to eliminate the system dependency and well protect the data/system boundry,which is also aligned with the DEVOPS,,both team could focus on their own domain/services\u0026amp;rsquo; development and maintenance.\nTechniques used Springboot + Spring cloud config, constrained by infra, services are running on in hourse VM instances. Quartz, based on java, easy to custom against requirements, also support clustering. Oracle database JWT is adopted for system to system authentication and authorization. Key Components Scheduling Platform The platform is implemented using Springboot + Quartz + JDBC, store the all the Quartz related configs in database, it support clustering and easy to build support portal to server better user experience on …","date":1620432000,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":700,"html":"To mark down a batch data sync accross system solution","keywords":null,"kind":"page","lang":"en","lastmod":1620432000,"objectID":"1f49b0a4d10236f01af572359f09a560","permalink":"http://akjamie.github.io/post/2021-05-08-microservice-data-sync/","publishdate":"2021-05-08T00:00:00Z","readingtime":3,"relpermalink":"/post/2021-05-08-microservice-data-sync/","section":"post","tags":["Microservice","Batch"],"title":"Microservice design - practice","type":"post","url":"/post/2021-05-08-microservice-data-sync/","weight":0,"wordcount":624},{"author":null,"categories":["Cloud"],"content":"Today i took Google Cloud Certified Professional Cloud Architect exam, the result is pass，through the final result is being reviewed for compliance with exam terms and conditions, i\u0026amp;rsquo;d like to summarize the preparation path i went through for this exam, hope that could help others who want to take the same exam.\nAbout the exam Professional Cloud Architect is one of the google cloud professonal cerficates, it requires a thorough understanding of cloud architecture and Google Cloud Platform, and verifies the capability of design, develop, and manage robust, secure, scalable, highly available, and dynamic solutions to drive business objectives. so it assesses the ability of:\nDesign and plan a cloud solution architecture Analyze and optimize technical and business processes Manage and provision the cloud solution infrastructure Manage implementations of cloud architecture Design for security and compliance Ensure solution and operations reliability The exam is 2 hours duration and …","date":1615248000,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":500,"html":"Experience sharing","keywords":null,"kind":"page","lang":"en","lastmod":1615248000,"objectID":"97aec582d92ad32a8139a48ea9ecea7d","permalink":"http://akjamie.github.io/post/2021-03-09-gcp-professional-cerified-architecture-prep/","publishdate":"2021-03-09T00:00:00Z","readingtime":2,"relpermalink":"/post/2021-03-09-gcp-professional-cerified-architecture-prep/","section":"post","tags":["GCP","Certificate"],"title":"How i prepare the Google Cloud Professional Cloud Architect Certificate","type":"post","url":"/post/2021-03-09-gcp-professional-cerified-architecture-prep/","weight":0,"wordcount":421},{"author":null,"categories":["Microservice"],"content":"this page shows how to run a spring boot application on local K8S environment.\nObjectives 1.create spring boot application and build a docker image\n2.create local persistent volume to share the files from host\n3.create deployment and service to run this application and expose for external access.\nBefore you begin install docker desktop in local and enable local k8s cluster, more details please refer to docker official guide or k8s local cluster setup guide.\nCreate spring boot application and build docker image in this demo, created a simple config service using spring boot cloud config, source code on git\nSpring boot application - config-service 1.use Spring Cloud Config to quickly implement a config service, please refer to spring cloud documents.\n2.create a service config repos on git, in this demo, we will use the native config rather than points to git directly as network delay caused by GFW.\nservice configs - spring: server: port: 8888 spring: application: name: config-service …","date":1606608000,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1000,"html":"to run a cloud config service which built using spring boot cloud config","keywords":null,"kind":"page","lang":"en","lastmod":1606608000,"objectID":"2a8f9c88286c4d7e3e1b6932d3971e64","permalink":"http://akjamie.github.io/post/2020-11-29-run-application-on-k8s/","publishdate":"2020-11-29T00:00:00Z","readingtime":5,"relpermalink":"/post/2020-11-29-run-application-on-k8s/","section":"post","tags":["Spring Cloud","K8S"],"title":"Kubernates - Practice","type":"post","url":"/post/2020-11-29-run-application-on-k8s/","weight":0,"wordcount":959},{"author":null,"categories":["Cloud"],"content":"To document the setup process for VPC peering accross different GCP project VPCs for demo purpose only.\nSetup VPC peering accross two GCP projects in this demo, will create two GCP projects, and setup VPC network peering to work as network bridge to make sure the vm in one of GCP project can access the MongoDB installed on VM of another GCP project. preparation Mongo backups 1\u0026amp;gt; backup MongoDB\nto use mongo shell command to backup data, samples as below\nmongodump --host 127.0.0.1 --port 27017 --out /Users/jamie/Documents/work-benches/mongo/dump/20200907 --gzip --collection users --db test 2\u0026amp;gt; install Google Cloud SDK in local\nplease refer to https://cloud.google.com/storage/docs/gsutil_install\noutput:\n2020-09-07T11:20:57.540+0800 writing test.users to\n2020-09-07T11:20:57.792+0800 done dumping test.users (10004 documents)\n3\u0026amp;gt; upload the backup file to Cloud Storage\ngsutil cp -r ../20200907 gs://mongo-backup-repo GCP resource creation Resource GCP project GCP network GCP Subnet …","date":1599436800,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":500,"html":"gcp network connections - VPC peering/VPN - poc","keywords":null,"kind":"page","lang":"en","lastmod":1599436800,"objectID":"fc669180747381671e5f6b7abc518fee","permalink":"http://akjamie.github.io/post/2020-09-07-gcp-network-connection/","publishdate":"2020-09-07T00:00:00Z","readingtime":2,"relpermalink":"/post/2020-09-07-gcp-network-connection/","section":"post","tags":["GCP","VPC","Mongo"],"title":"GCP Network Connection (1)","type":"post","url":"/post/2020-09-07-gcp-network-connection/","weight":0,"wordcount":408},{"author":null,"categories":["NoSQL"],"content":"Mongo installation(for test) docker command for local mongo installation docker run -p 27017:27017 -v /Users/jamie/Documents/work-benches/mongo/test:/data/db --name mongo -d mongo:latest check mongo running status docker ps | grep mongo\n9161bd4811a8 mongo:latest \u0026amp;ldquo;docker-entrypoint.s…\u0026amp;rdquo; 6 days ago Up 6 days 0.0.0.0:27017-\u0026amp;gt;27017/tcp mongo\nmongo Shell installation please refer to https://docs.mongodb.com/manual/tutorial/configure-mongo-shell/\nCRUD operations insertOne vs insertMany insertOne - since version 3.2\ninsertMany - since version 3.2\nMongoShell command format: samples to provide some samples and corresponded shell command for better understand \u0026amp;amp; master each params \u0026amp;amp; grammer\n1\u0026amp;gt; insert one document to inventory\ndb.inventory.insertOne({\u0026amp;#34;item\u0026amp;#34;: \u0026amp;#34;canvas\u0026amp;#34;, \u0026amp;#34;qty\u0026amp;#34;: 100, \u0026amp;#34;tags\u0026amp;#34;: [\u0026amp;#34;cotton\u0026amp;#34;], size: {\u0026amp;#34;h\u0026amp;#34;:28,\u0026amp;#34;w\u0026amp;#34;:100,\u0026amp;#34;uom\u0026amp;#34;: \u0026amp;#34;mm\u0026amp;#34;} }) 2\u0026amp;gt; insert many records in same command …","date":1599264000,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1200,"html":"Mongodb 基本操作 - Insert/Find/Update/Remove","keywords":null,"kind":"page","lang":"en","lastmod":1599264000,"objectID":"14760c47f9d32b9595709cb8f02ce592","permalink":"http://akjamie.github.io/post/2020-09-05-mongo-basics/","publishdate":"2020-09-05T00:00:00Z","readingtime":3,"relpermalink":"/post/2020-09-05-mongo-basics/","section":"post","tags":["NoSQL","Mongo"],"title":"Mongo DB Basics (1)","type":"post","url":"/post/2020-09-05-mongo-basics/","weight":0,"wordcount":1166},{"author":null,"categories":["Microservice"],"content":"先回顾上一篇sprint local transaction的内容，本地事务是用于指定资源，即单一数据源，其主要结构如下： 那么我们来看看spring的global transaction管理，其主要用于多数据源的业务场景中，如mysql + mysql, oracle + mysql + rabbitmq等，但是说到全局事务，我们需要先来谈谈xa。\nXA 与 JTA 什么是XA XA 是指由 X/Open 组织提出的分布式事务处理的规范. XA 规范主要定义了事务管理器(Transaction Manager)和局部资源管理器(Local Resource Manager)之间的接口. 目前，Oracle、Informix、DB2和Sybase等各大数据库厂家都提供对XA的支持。XA协议采用两阶段提交方式来管理分布式事务。 由如上的信息中我们能看到，在xa的规范中定义了如下3个方面：\nTransaction manager, 统筹管理多个resource manager Resource manager, 管理具体的XA resource Two-phase commit, 两阶段提交的方式\n4. Xid, 事务ID 用于区分事务 XA与JTA的联系 jta是xa规范在java中的实现 全局/JTA事务 JTA事务的两种支持方式 基于j2ee 应用服务器的jta，事务管理器和xa资源管理器均由应用服务器来管理，如jboss, ibm web sphere 采用非应用服务器的方式，如采用atomikos,Bitronix等 Atomikos 配置多数据源支持jta事务 环境准备：\n. 两台mysql数据库 主要配置：\n. datasource 配置文件\nspring: datasource: user: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/xa?allowPublicKeyRetrieval=true\u0026amp;amp;useSSL=false\u0026amp;amp;charset=utf8 username: xxx password: xxx audit: driver-class-name: com.mysql.cj.jdbc.Driver url: …","date":1567209600,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1900,"html":"Spring Transaction 之 JTA事务管理","keywords":null,"kind":"page","lang":"en","lastmod":1567209600,"objectID":"3e38b8eb6a18a4f74033eadce2edb3b4","permalink":"http://akjamie.github.io/post/2019-08-31-spring-global-transaction/","publishdate":"2019-08-31T00:00:00Z","readingtime":4,"relpermalink":"/post/2019-08-31-spring-global-transaction/","section":"post","tags":["Spring","Transaction Management"],"title":"Spring Transaction","type":"post","url":"/post/2019-08-31-spring-global-transaction/","weight":0,"wordcount":1823},{"author":null,"categories":["Microservice"],"content":"事务特性和隔离级别 事务基础 事务特性 - 原子性(Atomicity) 事务包含的所有操作是一个原子单元，要么全部成功，要么全部失败 - 一致性（Consistency） A给B转钱，A减和B增这两个操作必须保持一致 - 隔离性（Isolation） 事务中的数据可见性，不同隔离级别，确保了不同的可见性级别，防止脏读，幻读等 - 持久性（Durability） 事务操作结果将被持久化到存储磁盘上 事务的隔离级别 - Read Uncommitted，可以读取其它事务未完成的结果 - Read Committed，在该事务完成后，才能读取该事务的数据更新后的结果 - Repeatable Read，可以保证在整个事务的过程中，对同一笔数据的读取结果是相同的，不管其他事务是否同时在对同一笔数据进行更新，也不管其他事务对同一笔数 据的更新提交与否 - Serializable，最严格的事务隔离控制，类似于表级别锁，所有事务操作依次有序执行 SQL事务测试举例 - mysql 查看事务隔离级别, 默认是REPEATABLE-READ select @@global.transaction_isolation,@@transaction_isolation; 设置事务隔离级别 SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; level: { REPEATABLE READ | READ COMMITTED | READ UNCOMMITTED | SERIALIZABLE } 通过设置mysql的session或者全局事务隔离级别，来查看事务执行结果\n以上为基本的事务知识回顾，下面我们进入spring事务。\nSpring 事务 Spring并不会直接管理事务，而是提供了事务管理器，将事务管理的职责委托给JPA JDBC JTA DataSourceTransaction JMSTransactionManager 等框架提供的事务来实现。\nSpring支持两种事务，及全局事务和本地事务。\nLocal transaction 及原理 本地事务，指定resource的事务，跟应用服务器无关，如DataSourceTransactionManager\nspring事务的顶层接口 …","date":1567123200,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1700,"html":"Spring Transaction 之 Local Transaction实现原理","keywords":null,"kind":"page","lang":"en","lastmod":1567123200,"objectID":"e9b1e2c0b8cd6cae4ca10b6b1d7e3d6a","permalink":"http://akjamie.github.io/post/2019-08-30-spring-local-transaction/","publishdate":"2019-08-30T00:00:00Z","readingtime":4,"relpermalink":"/post/2019-08-30-spring-local-transaction/","section":"post","tags":["Spring","Transaction Management"],"title":"Spring Transaction","type":"post","url":"/post/2019-08-30-spring-local-transaction/","weight":0,"wordcount":1664},{"author":null,"categories":["NoSQL"],"content":"redis是一个key-value存储系统，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set \u0026amp;ndash;有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。\nRedis的优势与缺点: 优势 • 性能极高，redis读的速度110000次/秒，写的速度81000次/秒 • 丰富的数据类型，redis支持的类型String, List, Hash, Set 和Ordered Set • 原子性，redis的所有操作都是原子性的，多个操作也支持事务 • 丰富的特性，如pub/sub,key过期等 缺点 • 耗内存，内存占用过高 • 持久化，Redis直接将数据存储到内存中，要将数据保存到磁盘上，Redis可以使用两种方式实现持久化， 定时快照(snapshot)：每隔一段时间将整个数据库写到磁盘上，每次军写全部数据，代价非常高。 基于语句追加(aof): 只追踪变化的数据，但是追加的log可能过大，同时所有的操作均重新执行一边，回复速度慢。 Redis.conf 配置说明: • Daemonize no|yes 是否启用守护进程 • Pidfile /var/run/redis.pid • Port 6379 端口 redis监听端口 • Database 16 设置数据库的数目。 • loglevel notice # debug (适用于开发或测试阶段) # verbose (many rarely useful info, but not a mess like the debug level) # notice (适用于生产环境) # warning (仅仅一些重要的消息被记录) • 快照 - 存db到磁盘 - 根据给定的时间间隔和写入次数将数据保存到磁盘 • save 900 1 • save 300 10 • save 60 10000 …","date":1564876800,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":6000,"html":"redis 基础知识","keywords":null,"kind":"page","lang":"en","lastmod":1564876800,"objectID":"7a7657940b109fd5d226b9c2f64ee3db","permalink":"http://akjamie.github.io/post/2019-08-04-redis-basic/","publishdate":"2019-08-04T00:00:00Z","readingtime":12,"relpermalink":"/post/2019-08-04-redis-basic/","section":"post","tags":["Redis","NoSQL"],"title":"Redis foundation - I","type":"post","url":"/post/2019-08-04-redis-basic/","weight":0,"wordcount":5915},{"author":null,"categories":["Microservice"],"content":"用户场景 在某企业app上做Payment业务，在app端准备好request data， 如debit account number, credit account number, amount, notes, 点击submit。\n业务流程如下:\n用户提交payment 请求 server校验用户所在公司的每日转账限额(account level) 如果限额可以满足，则发起转账，否则reject请求\n问题和挑战 如何规避重复提交 如何避免分布式事务 解决思路 如何规避重复提交\n采用前端/后端生成UUID(it will be used as payment id), 需要缓存起来便于request进来后的校验， 当然也可以不缓存，只是校验这个ID是否已经存在，如果存在就reject请求。 如何避免分布式事务\n本例中，主要的分布式事务check point有3个，\n1)计算limit checking，在此过程中可能有新payment请求过来可能导致limit用尽或超支，这是业务上不能允许的;\n采用乐观锁,把锁控制在payment db里面而不是对entitlement db中的transaction limit进行CRUD，这样会使整个事务处理变得复杂; 且兵并发性能有大幅度提升\n2)payment data 写入transaction history db\nOracle RAC事务，用spring transaction 注解，@Transactional\n3)update B/E system response - payment status(succ/fail)\nOracle RAC事务，用spring transaction 注解，@Transactional 解决方案 一个方案proposal如下,主要针对limit checking部分加锁， payment data写入和status update 单独分开事务控制(主要由于如果B/E执行成功了，而update status failed导致payment数据回滚会导致数据丢失)\n处理流程diagram如下:\n事后思考 如果使用非oracle 集群，可能需要引入其他中间件来确保事务一致性，该部分思考中 上例中暂时缺少重试机制，如果payment status update failed， 是否可以再 …","date":1563910810,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":800,"html":"分享一个遇到的分布式事务case, 以及部分引申思考","keywords":null,"kind":"page","lang":"en","lastmod":1563910810,"objectID":"8330b93025c5aeced8363c4d3612ede8","permalink":"http://akjamie.github.io/post/2019-07-24-one-distributed-transaction-user-case/","publishdate":"2019-07-23T19:40:10Z","readingtime":2,"relpermalink":"/post/2019-07-24-one-distributed-transaction-user-case/","section":"post","tags":["Design","Distributed Transaction","Transaction Management"],"title":"微服务中的分布式事务","type":"post","url":"/post/2019-07-24-one-distributed-transaction-user-case/","weight":0,"wordcount":774},{"author":null,"categories":["Microservice"],"content":"Spring batch是轻量级，全面的批处理框架，旨在开发对企业系统日常运营至关重要的强大批处理应用程序。\nSpring Batch负责大量记录，包括日志记录/跟踪，事务管理，作业处理统计，作业重启，跳过和资源管理。 它还提供更高级的技术服务和功能，通过优化和分区技术实现极高容量和高性能的批处理作业。 简单和复杂的大批量批处理作业可以高度可扩展的方式利用框架来处理大量信息。\n关于spring batch的细节介绍，请参加Spring batch官网，本文主要是记录一个poc的过程，主要包含单个实例应用和远程分析多实例处理的cases。\n模拟demo的用户场景：\n定时处理多个系统输出的csv文件，然后跑批处理到数据到DB，简要架构图如下:\n单个实例处理(且单个文件) 通过cron job定时调度batch job，整个job由两个step组成的一个flow，step1 - 清理目标数据库表(模拟场景，可能是数据备份，建立新的day表等); step2 - 加载文件，解析数据内容并转化为java对象,经processor处理，后倒入db存储. sequence diagram:\n主要配置文件\nSingleProcessConfiguration package org.akj.batch.configuration; import lombok.extern.slf4j.Slf4j; import org.akj.batch.constant.Constant; import org.akj.batch.entity.Person; import org.akj.batch.listener.CustomJobExcecutionListener; import org.akj.batch.processor.PersonItemProcessor; import org.akj.batch.repository.PeopleRepository; import org.springframework.batch.core.Job; import org.springframework.batch.core.Step; import org.springframework.batch.core.StepContribution; import …","date":1563888970,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1900,"html":"spring batch 批处理poc","keywords":null,"kind":"page","lang":"en","lastmod":1563888970,"objectID":"8490cf286f8f2f80867d09f757b5b4de","permalink":"http://akjamie.github.io/post/2019-07-21-spring-batch-single-process-remote-partitioning/","publishdate":"2019-07-23T13:36:10Z","readingtime":4,"relpermalink":"/post/2019-07-21-spring-batch-single-process-remote-partitioning/","section":"post","tags":["Spring Batch"],"title":"Spring batch - single process \u0026 remote partitioning(multi instances)","type":"post","url":"/post/2019-07-21-spring-batch-single-process-remote-partitioning/","weight":0,"wordcount":1804},{"author":null,"categories":["Microservice"],"content":"Demo的介绍 本文主要通过一个小demo，来综合运用spring cloud系列技术，demo中主要包含如下模块/组建：\nAuthorization service, 基于Spring Security Oauth2 + Spring Cloud Oauth2,提供集中授权服务,如Token 生成,校验等 API gateway, edge service, 提供统一的对外访问接口，集成了oauth2 安全校验，Netflix Hystrix服务降级，依赖隔离，断路保护功能 Service registry,基于Netflix Eureka的注册中心提供服务注册和服务发现功能 Config service, 采用了 Spring cloud config + Spring Cloud Bus提供简单的配置中心 Catalog service, 示例服务，模拟一个简单的商品服务目录，仅一维结构 Inventory service, 示例服务, 模拟简单的商品库存服务 Hystrix dashboard, 聚合hystrix的metrics监控展示 整体架构 Spring boot 项目列表 authorization-service，https://github.com/cloud-poc/authorization-service backed by mysql database cloud-api-gateway，https://github.com/cloud-poc/api-gateway 目前主要集成了oauth2 jwt授权认证，无valid token直接reject service-registry，https://github.com/cloud-poc/service-registry eureka 服务端，外置话了部分参数，便于在docker/docker-compose部署时根据需要配置 config-service，https://github.com/cloud-poc/config-service 基于spring cloud config的配置中心，后续会考虑用携程apollo替换 catalog-service， https://github.com/cloud-poc/catalog-service 模拟一个简单的商品服务目录，用 …","date":1563888970,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1400,"html":"通过一个demo，来综合运用spring cloud系列技术","keywords":null,"kind":"page","lang":"en","lastmod":1563888970,"objectID":"9b3e9026272f83b7fa03bbc655d7052a","permalink":"http://akjamie.github.io/post/2019-07-15-spring-cloud-demo-overview/","publishdate":"2019-07-23T13:36:10Z","readingtime":3,"relpermalink":"/post/2019-07-15-spring-cloud-demo-overview/","section":"post","tags":["Spring Cloud","Spring Boot","Docker"],"title":"Spring Cloud Series","type":"post","url":"/post/2019-07-15-spring-cloud-demo-overview/","weight":0,"wordcount":1395},{"author":null,"categories":["Others"],"content":"Background Let\u0026amp;rsquo;s Encrypt is a certificate authority that provides X.509 certificates for Transport Layer Security (TLS) encryption at no charge,The certificate is valid for 90 days, during which renewal can take place at anytime. 这样我们就可以用上免费的CA cert来安全expose我们自己的网站或者服务\n基本的http和https知识请阅读https://linuxstory.org/deploy-lets-encrypt-ssl-certificate-with-certbot/的‘背景知识’部分，作者讲述的非常很不错。\nObjectives 通过例子来demo如何生成和使用Internet Security Research Group推出的Let’s Encrypt 免费证书 主要涉及如下:\nDocker, docker-compose用来部署nginx Certbot，用来为域名生成CA证书 Not In Scope Docker和docker compose的相关概念和安装，请参考docker官方文档\nSteps to configure certs Generate CA cert 安装Certbot 客户端 wget https://dl.eff.org/certbot-auto chmod a+x ./certbot-auto ./certbot-auto --help 验证域名所有权 该步骤需要启动nginx:\na. 准备docker-compose file\nversion: \u0026amp;#39;3.0\u0026amp;#39; services: nginx: restart: always image: nginx:1.15.6 ports: - 80:80 - 443:443 volumes: - ./conf.d:/etc/nginx/conf.d - ./log:/var/log/nginx - ./wwwroot:/var/www - /etc/letsencrypt:/etc/letsencrypt Docker …","date":1563129005,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1500,"html":"介绍用如何使用Let's Encrypt's Certbot生成CA证书，并配置在nginx中保护站点","keywords":null,"kind":"page","lang":"en","lastmod":1563129005,"objectID":"1e99a9399c856f918b86e98ea630ce98","permalink":"http://akjamie.github.io/2019/07/14/free-https-certs-docker-nginx/","publishdate":"2019-07-14T18:30:05Z","readingtime":3,"relpermalink":"/2019/07/14/free-https-certs-docker-nginx/","section":"post","tags":["Docker","Let’s Encrypt","Nginx"],"title":"Free CA Certs setup using Certbot + Docker + Nginx","type":"post","url":"/2019/07/14/free-https-certs-docker-nginx/","weight":0,"wordcount":1470},{"author":null,"categories":["Others"],"content":"背景介绍 很多时候我们由于great firewall，我们无法访问某些网站，这时候就需要用到网络代理来应对一些比较urgent的case，本文主要会介绍如何使用Shadowsocks 和 Proxychain来 setup linux network proxy，这样在命令行下都可以起到代理和转发的作用，而不仅限于浏览器的代理。\nSS server \u0026amp;amp; SS local的配置 预置条件 Installed Python 3 + pip 安装命令如下:\nsudo apt-get install python3-pip 如果得到no packages found - python3-pip的错误，请添加相应的apt sources. 修改/etc/apt/sources.list文件，添加如下4条source信息\ndeb http://cn.archive.ubuntu.com/ubuntu bionic main multiverse restricted universe deb http://cn.archive.ubuntu.com/ubuntu bionic-updates main multiverse restricted universe deb http://cn.archive.ubuntu.com/ubuntu bionic-security main multiverse restricted universe deb http://cn.archive.ubuntu.com/ubuntu bionic-proposed main multiverse restricted universe Installed Shadowsocks 一台可以访问__外网__的云主机，Azure/AWS/Alicloud-HK等都可以 sudo pip install shadowsocks SSServer的配置和启动 在云主机上创建sss 配置文件shadowsocks.json(推荐放置在/etc/shadowsocks/ 下面)，其内容如下：\n{ \u0026amp;#34;server\u0026amp;#34;:\u0026amp;#34;0.0.0.0\u0026amp;#34;, \u0026amp;#34;server_port\u0026amp;#34;:\u0026amp;#34;{port}\u0026amp;#34;, …","date":1563115852,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1500,"html":"Using Shadowsocks+proxychain to build network proxy for personal learning purpose","keywords":null,"kind":"page","lang":"en","lastmod":1563115852,"objectID":"9ba7a47fc53cf70ad5cba2da0e710040","permalink":"http://akjamie.github.io/post/2019-07-14-linux-network-proxy-setup-desktop-terminal/","publishdate":"2019-07-14T14:50:52Z","readingtime":3,"relpermalink":"/post/2019-07-14-linux-network-proxy-setup-desktop-terminal/","section":"post","tags":["Shadowsocks","Proxychain"],"title":"Linux network proxy setup for desktop and terminal","type":"post","url":"/post/2019-07-14-linux-network-proxy-setup-desktop-terminal/","weight":0,"wordcount":1441},{"author":null,"categories":["Others"],"content":"Install vault Prepare docker-compose.yml, content as below (just a sample here, please don\u0026amp;rsquo;t use it for production, there are more aspects should be considered)\nversion: \u0026amp;#39;3\u0026amp;#39; services: consul: container_name: consul.server command: agent -server -bind 0.0.0.0 -client 0.0.0.0 -bootstrap-expect=1 image: consul:latest restart: always volumes: - ./consul.server/config:/consul/config - ./consul.server/data:/consul/data ports: - \u0026amp;#34;9300:9300\u0026amp;#34; - \u0026amp;#34;9500:9500\u0026amp;#34; - \u0026amp;#34;9600:9600/udp\u0026amp;#34; vault: container_name: vault.server image: vault ports: - \u0026amp;#34;8200:8200\u0026amp;#34; restart: always links: - consul:consul.server volumes: - ./vault.server/config:/mnt/vault/config - ./vault.server/data:/mnt/vault/data - ./vault.server/logs:/mnt/vault/logs - ./vault.server/config:/vault/config - /etc/letsencrypt:/etc/letsencrypt cap_add: - IPC_LOCK command: server consul config - config.json { \u0026amp;#34;datacenter\u0026amp;#34;: \u0026amp;#34;data-center-1\u0026amp;#34;, \u0026amp;#34;node_name\u0026amp;#34;: \u0026amp;#34;master-node\u0026amp;#34;, …","date":1563106227,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1000,"html":"介绍用docker-compose安装Vault并配置nginx代理，并与spring cloud application进行集成","keywords":null,"kind":"page","lang":"en","lastmod":1563106227,"objectID":"d835fd77681a15b3b438bf8c8fb74fe4","permalink":"http://akjamie.github.io/2019/07/14/harshicorp-vault-setup-config/","publishdate":"2019-07-14T12:10:27Z","readingtime":2,"relpermalink":"/2019/07/14/harshicorp-vault-setup-config/","section":"post","tags":["Harshicorp Vault","Docker","Spring Cloud"],"title":"Harshicorp Vault Setup Config and integration with spring boot","type":"post","url":"/2019/07/14/harshicorp-vault-setup-config/","weight":0,"wordcount":938},{"author":null,"categories":null,"content":"\rJamie Zhang is from NorthWestern of China - Shaanxi province. He has 10+ years of experience in web/mobile application development and management, led a number of large-scale projects in big companies, e.g. Huawei, ChinaSoft, HSBC.\nHe is experienced Agile Coach, and facilitated team to adopt scrum , maturer scrum practices and built their mindset of keeping improving and fast-iteration + fast-fail, which built good team culture, improved project delivery efficiency and great customer/market feedback.\nHe is enthusiastic about the open source and emerging technologies in the field of Microservice, Cloud native, Artificial Intelligence, etc.\nHe is happy with his current job, and also willing to take new challenges if the opportunity matches his career path.\nFeel free to connect Jamie via Github (https://github.com/akjamie), drop him an email (akjamie.zhang@outlook.com) or leave message on LinkedIn(https://www.linkedin.com/in/jamie-zhang/).\n","date":1563098046,"dir":"\\","expirydate":-62135596800,"fuzzywordcount":200,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1563098046,"objectID":"6083a88ee3411b0d17ce02d738f69d47","permalink":"http://akjamie.github.io/about/","publishdate":"2019-07-14T09:54:06Z","readingtime":1,"relpermalink":"/about/","section":"","tags":null,"title":"Jamie","type":"page","url":"/about/","weight":0,"wordcount":133},{"author":null,"categories":null,"content":"\rJamie Zhang is from NorthWestern of China - Shaanxi province. He has 10+ years of experience in web/mobile application development and management, led a number of large-scale projects in big companies, e.g. Huawei, ChinaSoft, HSBC.\nHe is experienced Agile Coach, and facilitated team to adopt scrum , maturer scrum practices and built their mindset of keeping improving and fast-iteration + fast-fail, which built good team culture, improved project delivery efficiency and great customer/market feedback.\nHe is enthusiastic about the open source and emerging technologies in the field of Microservice, Cloud native, Artificial Intelligence, etc.\nHe is happy with his current job, and also willing to take new challenges if the opportunity matches his career path.\nFeel free to connect Jamie via Github (https://github.com/akjamie), drop him an email (akjamie.zhang@outlook.com) or leave message on LinkedIn(https://www.linkedin.com/in/jamie-zhang/).\n","date":1563098046,"dir":"top\\","expirydate":-62135596800,"fuzzywordcount":200,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1563098046,"objectID":"7a0da914bde98a7d3fcc7f6c5886633c","permalink":"http://akjamie.github.io/top/about/","publishdate":"2019-07-14T09:54:06Z","readingtime":1,"relpermalink":"/top/about/","section":"top","tags":null,"title":"Jamie","type":"top","url":"/top/about/","weight":0,"wordcount":133},{"author":null,"categories":["Cloud"],"content":"AWS SAM - lambda demo Demo User Case User requirement: dynamically get regional mobile app feature availability status,status data is kept in AWS dynamodb.\nProjectcodes hierarchical structure presented as below\n├── README.md -- This instructions file ├── event.json -- API Gateway Proxy Integration event payload ├── feature_availability -- Source code for a lambda function │ ├── __init__.py │ ├── app.py -- Lambda function code │ ├── requirements.txt -- Lambda function code - dependencies list ├── template.yaml -- SAM Template └── tests -- Unit tests ├── __init__.py └── app_test.py Business Flow Prerequisites AWS CLI already configured with Administrator permission Python 3 installed Docker installed\nSetup process Local development Invoking function locally using a local sample payload\nsam local invoke GetEntityConfigFunction --event event.json Invoking function locally through local API Gateway\nsam local start-api If the previous command ran successfully you should now be able to hit …","date":1560173770,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":900,"html":"基于python，演示AWS SAM的开发部署过程，含通过aws api gateway暴露api","keywords":null,"kind":"page","lang":"en","lastmod":1560173770,"objectID":"4750e3381ec58808af59b35d9c9133fb","permalink":"http://akjamie.github.io/post/2019-07-23-aws-server-application-model/","publishdate":"2019-06-10T13:36:10Z","readingtime":2,"relpermalink":"/post/2019-07-23-aws-server-application-model/","section":"post","tags":["Python","AWS","Serveless"],"title":"AWS Serveless Application Model","type":"post","url":"/post/2019-07-23-aws-server-application-model/","weight":0,"wordcount":897},{"author":null,"categories":["Others"],"content":"UIPath是一个非常不错的RPA(Robotic Process Automation) 工具和平台，并且有开源社区版本可以方便RPA爱好者去尝试，本文是介绍如何在UIPath下调用python的脚本中的方法并提取返回值\n模拟问题场景 uipath加载python脚本，并根据传入的配置文件，返回配置文件中的符合条件的配置信息\n解决步骤 预置条件： a. python脚本pmt_detection.py， 内容如下：\nfrom configparser import ConfigParser def getPmtReqsStatToBeSync(configFilePath): \u0026amp;#34;\u0026amp;#34;\u0026amp;#34; read config item in rpa-config.ini :param: configFilePath - config file :return: PMT.sync_scan_on_status \u0026amp;#34;\u0026amp;#34;\u0026amp;#34; cfg = ConfigParser() cfg.read(configFilePath) return cfg.get(\u0026amp;#34;PMT\u0026amp;#34;, \u0026amp;#34;sync_scan_on_status\u0026amp;#34;).split(\u0026amp;#39;,\u0026amp;#39;) b. 配置文件rpa-config.ini,内容如下：\n[PMT] sync_scan_on_status=SUBMITTED,INPROGRESS rm_scan_on_status=CANCELLED,COMPLETED [XXX] doc_type_list=ID,PASSPORT 我们本次的目标是，用getPmtReqsStatToBeSync去拿到配置文件中的key sync_scan_on_status对应的value，即SUBMITTED,INPROGRESS\nc.uipath项目已创建且python activities已安装\n设置PYTHON_HOME并用变量保存 避免每次调用python脚本的时候都要hardcode python env home 地址，及python.exe所在的目录\na. 我们现在系统环境变量中设置PYTHON_HOME,如下图 b. 在UIpath activities中找到\u0026amp;rsquo;Get Environment …","date":1547899827,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":1400,"html":"simple demostration about how to implement python script invocation in UIPath","keywords":null,"kind":"page","lang":"en","lastmod":1547899827,"objectID":"5e1a3898b14e397cf43e62014d898949","permalink":"http://akjamie.github.io/2019/01/19/UIPath-invoke-python-method/","publishdate":"2019-01-19T12:10:27Z","readingtime":3,"relpermalink":"/2019/01/19/UIPath-invoke-python-method/","section":"post","tags":["Python","UIPath"],"title":"How to invoke python scripts in UIPath","type":"post","url":"/2019/01/19/UIPath-invoke-python-method/","weight":0,"wordcount":1345},{"author":null,"categories":null,"content":"","date":-62135596800,"dir":"search\\","expirydate":-62135596800,"fuzzywordcount":100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8946788897930c0c0c39fbfcd30ff2e4","permalink":"http://akjamie.github.io/search/placeholder/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/search/placeholder/","section":"search","tags":null,"title":"","type":"search","url":"/search/placeholder/","weight":0,"wordcount":0},{"author":null,"categories":["Microservice"],"content":"What is JPA? \u0026amp;ldquo;The Java Persistence API is the Java API for the management of persistence and object/relational mapping in Java EE and Java SE environments. It provides an object/relational mapping facility for the Java application developer using a Java domain model to manage a relational database.\u0026amp;rdquo;\nIt\u0026amp;rsquo;s quoted from JSR(Java Specification Request) managed by Java Community Process.\nThis is just the definition of what\u0026amp;rsquo;s JPA, let\u0026amp;rsquo;s recall how we persist data before adopting JPA.\nLoading jdbc driver Class.forName(\u0026amp;#34;com.mysql.jdbc.driver\u0026amp;#34;) Get Connection Connection conn = DriverManager.getConnection(url,username, password) Create Statement or PreparedStatement PreparedStatement stmt = conn.prepareStatement(sql); # Set args stmt.setXx Execute Query or Update ResultSet rs = stmt.executeQuery() # Iterator the rs to manual map to Java Domain object stmt.executeUpdate() # for insert, delete or update Seen from above steps, the difficulty parts are the …","date":-62135596800,"dir":"post\\","expirydate":-62135596800,"fuzzywordcount":300,"html":"","keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"59fc7f378f27ac7f2a8c309bf8b81d37","permalink":"http://akjamie.github.io/post/2022-07-08-spring-data-series-01/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/post/2022-07-08-spring-data-series-01/","section":"post","tags":["Spring Data","JPA"],"title":"Spring Data \u0026 Data Persistent","type":"post","url":"/post/2022-07-08-spring-data-series-01/","weight":0,"wordcount":265}]