<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Jamie&#39;s Blog</title>
    <link>http://akjamie.github.io/categories/ai/</link>
    <description>Recent content in AI on Jamie&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Dec 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://akjamie.github.io/categories/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Complete Guide to Building Agentic AI Systems: From Basics to Production</title>
      <link>http://akjamie.github.io/post/2025-12-11-build-agentic-ai/</link>
      <pubDate>Thu, 11 Dec 2025 00:00:00 +0000</pubDate>
      <guid>http://akjamie.github.io/post/2025-12-11-build-agentic-ai/</guid>
      <description>&lt;h1 id=&#34;building-production-agentic-ai-a-tech-leads-guide&#34;&gt;Building Production Agentic AI: A Tech Lead&amp;rsquo;s Guide&lt;/h1&gt;&#xA;&lt;h2 id=&#34;why-this-guide-matters&#34;&gt;Why This Guide Matters&lt;/h2&gt;&#xA;&lt;p&gt;If you&amp;rsquo;ve shipped LLM-based features, you&amp;rsquo;ve probably hit the wall where simple prompt engineering stops working. Your PM wants the AI to &amp;ldquo;do more,&amp;rdquo; your team is drowning in edge cases, and you&amp;rsquo;re not sure if you need better prompts, more tools, or an entirely different architecture.&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve been there. After building several production agentic systems, here&amp;rsquo;s what I wish someone had told me on day one: &lt;strong&gt;The difference between teams that ship working AI agents and those that don&amp;rsquo;t isn&amp;rsquo;t the model—it&amp;rsquo;s the discipline around evaluation and error analysis.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LangChain 1.x Architecture Guide for Tech Leads and Solution Architects</title>
      <link>http://akjamie.github.io/post/2025-12-08-langchain-1.x-architecture/</link>
      <pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate>
      <guid>http://akjamie.github.io/post/2025-12-08-langchain-1.x-architecture/</guid>
      <description>&lt;h1 id=&#34;the-complete-langchain-1x-architecture-guide-for-tech-leads-and-solution-architects&#34;&gt;The Complete LangChain 1.x Architecture Guide for Tech Leads and Solution Architects&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;A Production-Ready Blueprint for Building Enterprise AI Agent Solutions&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Last Updated: December 2025 | Reading Time: 25 minutes&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;If you&amp;rsquo;re a tech lead or solution architect evaluating LangChain for your next AI agent project, this guide is for you. LangChain 1.x represents a significant maturation of the framework, shifting from experimental prototypes to production-ready agent systems. This isn&amp;rsquo;t just another tutorial—it&amp;rsquo;s a comprehensive architectural deep-dive designed to help you make informed decisions about building scalable AI solutions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning Notes: Fine-Tuning Transformer Models</title>
      <link>http://akjamie.github.io/post/2025-09-02-learning-notes-transformer-finetune/</link>
      <pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://akjamie.github.io/post/2025-09-02-learning-notes-transformer-finetune/</guid>
      <description>&lt;h1 id=&#34;fine-tuning-transformer-models-a-beginners-guide&#34;&gt;Fine-Tuning Transformer Models: A Beginner&amp;rsquo;s Guide&lt;/h1&gt;&#xA;&lt;p&gt;Fine-tuning is the process of adapting a pre-trained transformer model (like GPT, BERT, etc.) to a specific task or dataset. This blog will help you understand the basics, workflow, and best practices for fine-tuning.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-fine-tuning&#34;&gt;What is Fine-Tuning?&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Pre-trained models&lt;/strong&gt; learn general language patterns from massive datasets.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt; adapts these models to your specific task (e.g., sentiment analysis, Q&amp;amp;A, summarization) using a smaller, task-specific dataset.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div class=&#34;mermaid&#34;&gt;&#xA;  &#xD;&#xA;flowchart LR&#xD;&#xA;    A[Pre-trained Model] --&gt; B[Fine-tuning Process]&#xD;&#xA;    C[Task-specific Dataset] --&gt; B&#xD;&#xA;    B --&gt; D[Fine-tuned Model]&#xD;&#xA;&#xA;&lt;/div&gt;&#xA;&lt;script&gt;&#xA;  function loadMermaid() {&#xA;    if (typeof mermaid === &#39;undefined&#39;) {&#xA;      var script = document.createElement(&#39;script&#39;);&#xA;      script.src = &#39;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js&#39;;&#xA;      script.onload = function() {&#xA;        &#xA;        mermaid.initialize({&#xA;          startOnLoad: true,&#xA;          theme: &#39;default&#39;,&#xA;          flowchart: {&#xA;            useMaxWidth: true,&#xA;            htmlLabels: true&#xA;          }&#xA;        });&#xA;        &#xA;        &#xA;        const mermaidElements = document.querySelectorAll(&#39;.mermaid&#39;);&#xA;        if (mermaidElements.length &gt; 0) {&#xA;          mermaid.init(mermaidElements);&#xA;        }&#xA;      };&#xA;      &#xA;      &#xA;      script.onerror = function(e) {&#xA;        console.error(&#39;Failed to load Mermaid library&#39;, e);&#xA;      };&#xA;      &#xA;      document.head.appendChild(script);&#xA;    } else {&#xA;      &#xA;      const mermaidElements = document.querySelectorAll(&#39;.mermaid&#39;);&#xA;      if (mermaidElements.length &gt; 0) {&#xA;        mermaid.init(mermaidElements);&#xA;      }&#xA;    }&#xA;  }&#xA;  &#xA;  &#xA;  if (document.readyState === &#39;loading&#39;) {&#xA;    document.addEventListener(&#39;DOMContentLoaded&#39;, loadMermaid);&#xA;  } else {&#xA;    &#xA;    window.setTimeout(loadMermaid, 0);&#xA;  }&#xA;&lt;/script&gt;&#xA;&lt;h2 id=&#34;why-fine-tune&#34;&gt;Why Fine-Tune?&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Saves time and resources (no need to train from scratch)&lt;/li&gt;&#xA;&lt;li&gt;Leverages powerful language understanding&lt;/li&gt;&#xA;&lt;li&gt;Achieves state-of-the-art results on custom tasks&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;typical-fine-tuning-workflow&#34;&gt;Typical Fine-Tuning Workflow&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Choose a Pre-trained Model&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Popular choices: GPT, BERT, RoBERTa, T5, etc.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Prepare Your Dataset&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Format data for your task (classification, generation, etc.)&lt;/li&gt;&#xA;&lt;li&gt;Clean and split into train/validation sets&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Set Up Training Environment&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use frameworks like Hugging Face Transformers, PyTorch, TensorFlow&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Configure Hyperparameters&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Learning rate, batch size, epochs, etc.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Train the Model&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Monitor loss and accuracy&lt;/li&gt;&#xA;&lt;li&gt;Use early stopping to prevent overfitting&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Evaluate &amp;amp; Test&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Check performance on validation/test data&lt;/li&gt;&#xA;&lt;li&gt;Adjust and retrain if needed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Deploy&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Integrate into your application or workflow&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;div class=&#34;mermaid&#34;&gt;&#xA;  &#xD;&#xA;flowchart TB&#xD;&#xA;    A[Choose Model] --&gt; B[Prepare Dataset]&#xD;&#xA;    B --&gt; C[Set Up Environment]&#xD;&#xA;    C --&gt; D[Configure Hyperparameters]&#xD;&#xA;    D --&gt; E[Train Model]&#xD;&#xA;    E --&gt; F[Evaluate &amp; Test]&#xD;&#xA;    F --&gt; G[Deploy]&#xD;&#xA;    G --&gt; H{Satisfied with Results?}&#xD;&#xA;    H --&gt;|No| D&#xD;&#xA;    H --&gt;|Yes| I[Complete]&#xD;&#xA;&#xA;&lt;/div&gt;&#xA;&lt;script&gt;&#xA;  function loadMermaid() {&#xA;    if (typeof mermaid === &#39;undefined&#39;) {&#xA;      var script = document.createElement(&#39;script&#39;);&#xA;      script.src = &#39;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js&#39;;&#xA;      script.onload = function() {&#xA;        &#xA;        mermaid.initialize({&#xA;          startOnLoad: true,&#xA;          theme: &#39;default&#39;,&#xA;          flowchart: {&#xA;            useMaxWidth: true,&#xA;            htmlLabels: true&#xA;          }&#xA;        });&#xA;        &#xA;        &#xA;        const mermaidElements = document.querySelectorAll(&#39;.mermaid&#39;);&#xA;        if (mermaidElements.length &gt; 0) {&#xA;          mermaid.init(mermaidElements);&#xA;        }&#xA;      };&#xA;      &#xA;      &#xA;      script.onerror = function(e) {&#xA;        console.error(&#39;Failed to load Mermaid library&#39;, e);&#xA;      };&#xA;      &#xA;      document.head.appendChild(script);&#xA;    } else {&#xA;      &#xA;      const mermaidElements = document.querySelectorAll(&#39;.mermaid&#39;);&#xA;      if (mermaidElements.length &gt; 0) {&#xA;        mermaid.init(mermaidElements);&#xA;      }&#xA;    }&#xA;  }&#xA;  &#xA;  &#xA;  if (document.readyState === &#39;loading&#39;) {&#xA;    document.addEventListener(&#39;DOMContentLoaded&#39;, loadMermaid);&#xA;  } else {&#xA;    &#xA;    window.setTimeout(loadMermaid, 0);&#xA;  }&#xA;&lt;/script&gt;&#xA;&lt;h2 id=&#34;example-fine-tuning-with-hugging-face-transformers&#34;&gt;Example: Fine-Tuning with Hugging Face Transformers&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;from&lt;/span&gt; transformers &lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model_name &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;bert-base-uncased&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; AutoModelForSequenceClassification&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;from_pretrained(model_name)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; AutoTokenizer&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;from_pretrained(model_name)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# Prepare your dataset (as Hugging Face Dataset object)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# ...&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;training_args &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; TrainingArguments(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    output_dir&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;./results&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    num_train_epochs&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;3&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    per_device_train_batch_size&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;8&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    evaluation_strategy&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;epoch&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    save_steps&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;10_000&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    save_total_limit&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;trainer &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; Trainer(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    model&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;model,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    args&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;training_args,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    train_dataset&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;train_dataset,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    eval_dataset&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;eval_dataset,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;trainer&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;train()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;parameter-efficient-fine-tuning-peft-techniques&#34;&gt;Parameter-Efficient Fine-Tuning (PEFT) Techniques&lt;/h2&gt;&#xA;&lt;p&gt;As transformer models grow larger, full fine-tuning becomes computationally expensive. Parameter-efficient fine-tuning (PEFT) methods address this by only training a small subset of parameters while freezing the rest of the model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding Transformer Architecture: The Foundation of Modern AI</title>
      <link>http://akjamie.github.io/post/2025-08-31-learning-notes-transformer/</link>
      <pubDate>Sun, 31 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://akjamie.github.io/post/2025-08-31-learning-notes-transformer/</guid>
      <description>&lt;h1 id=&#34;introduction-to-transformer-architecture&#34;&gt;Introduction to Transformer Architecture&lt;/h1&gt;&#xA;&lt;p&gt;In the rapidly evolving world of artificial intelligence, few innovations have been as transformative as the &lt;strong&gt;Transformer architecture&lt;/strong&gt;. Introduced in the seminal 2017 paper &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;&amp;ldquo;Attention is All You Need&amp;rdquo;&lt;/a&gt; by Vaswani et al., Transformers have become the backbone of virtually all state-of-the-art language models, including GPT-4, ChatGPT, and Google&amp;rsquo;s Bard.&lt;/p&gt;&#xA;&lt;p&gt;But what exactly is a Transformer, and why has it revolutionized natural language processing? In this comprehensive guide, we&amp;rsquo;ll break down the Transformer architecture from the ground up, using clear explanations and visual diagrams to help you understand how these powerful models work.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Run LLM on Intel Iris Xe GPU Using IEPX-LLM &#43; Ollama</title>
      <link>http://akjamie.github.io/post/2024-06-15-run-llm-on-intel-igpu/</link>
      <pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://akjamie.github.io/post/2024-06-15-run-llm-on-intel-igpu/</guid>
      <description>&lt;p&gt;In the realm of AI and machine learning, optimizing performance often hinges on utilizing GPU acceleration. However, Intel GPUs traditionally have not supported running AI workloads directly with popular frameworks like TensorFlow or PyTorch [5]. To address this, developers can turn to IEPX-LLM, a specialized library tailored for Intel&amp;rsquo;s XPU architecture.&lt;/p&gt;&#xA;&lt;h1 id=&#34;steps-to-run-llm-on-intel-gpu-with-iepx-llm--ollama-locally&#34;&gt;Steps to Run LLM on Intel GPU with IEPX-LLM + Ollama Locally&lt;/h1&gt;&#xA;&lt;h2 id=&#34;install-prerequisites&#34;&gt;Install Prerequisites&lt;/h2&gt;&#xA;&lt;h3 id=&#34;optional-update-gpu-driver&#34;&gt;(Optional) Update GPU Driver&lt;/h3&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Tips&#xA;:information_source:&#xA;It is recommended to update your GPU driver, if you have driver version lower than 31.0.101.5122. Refer to here for more information.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
