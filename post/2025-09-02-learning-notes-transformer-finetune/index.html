<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <meta property="og:site_name" content="Jamie&#39;s Blog">
    <meta property="og:type" content="article">

    
    <meta property="og:image" content="img/ai-01.jpg">
    <meta property="twitter:image" content="img/ai-01.jpg" />
    

    
    <meta name="title" content="Learning Notes: Fine-Tuning Transformer Models" />
    <meta property="og:title" content="Learning Notes: Fine-Tuning Transformer Models" />
    <meta property="twitter:title" content="Learning Notes: Fine-Tuning Transformer Models" />
    

    
    <meta name="description" content="A beginner-friendly guide to fine-tuning transformer models, including practical steps, tips, and common pitfalls.">
    <meta property="og:description" content="A beginner-friendly guide to fine-tuning transformer models, including practical steps, tips, and common pitfalls." />
    <meta property="twitter:description" content="A beginner-friendly guide to fine-tuning transformer models, including practical steps, tips, and common pitfalls." />
    

    <meta property="og:url" content="http://akjamie.github.io/post/2025-09-02-learning-notes-transformer-finetune/" />

    
    <meta property="twitter:card" content="summary" />
    
    

    <meta name="keyword"  content="&#34;Jamie Zhang, 博客, 个人网站, 互联网, Web, 云原生, Kubernetes, 微服务, Microservice&#34;">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>Learning Notes: Fine-Tuning Transformer Models | Jamie&#39;s Blog</title>

    <link rel="canonical" href="/post/2025-09-02-learning-notes-transformer-finetune/">

    

    
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    
    <link rel="stylesheet" href="/css/hugo-theme-cleanwhite.min.css">

    
    <link rel="stylesheet" href="/css/zanshang.min.css">

    
    <link rel="stylesheet" href="/css/font-awesome.all.min.css">

    
    

    
    <script src="/js/jquery.min.js"></script>

    
    <script src="/js/bootstrap.min.js"></script>

    
    <script src="/js/hux-blog.min.js"></script>

    
    <script src="/js/lazysizes.min.js"></script>

    
    

</head>




<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
        
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Jamie&#39;s Blog</a>
        </div>

        
        
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">All Posts</a>
                    </li>
                    
                        
                        <li>
                            <a href="/categories/ai/">ai</a>
                        </li>
                        
                        <li>
                            <a href="/categories/cloud/">cloud</a>
                        </li>
                        
                        <li>
                            <a href="/categories/deep-learning/">deep learning</a>
                        </li>
                        
                        <li>
                            <a href="/categories/microservice/">microservice</a>
                        </li>
                        
                        <li>
                            <a href="/categories/nosql/">nosql</a>
                        </li>
                        
                        <li>
                            <a href="/categories/others/">others</a>
                        </li>
                        
                    
                    
		    
		            <li>
                        <a href="/search"><i class="fa fa-search"></i></a>
		           </li>
                </ul>
            </div>
        </div>
        
    </div>
    
</nav>
<script>
    
    
    
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        
            $navbar.className = " ";
            
            setTimeout(function(){
                
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>




<style type="text/css">
    header.intro-header {
        background-image: url('/img/ai-01.jpg')
    }
</style>

<header class="intro-header" >

    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/llm" title="LLM">
                            LLM
                        </a>
                        
                        <a class="tag" href="/tags/transformer" title="Transformer">
                            Transformer
                        </a>
                        
                        <a class="tag" href="/tags/fine-tune" title="Fine-tune">
                            Fine-tune
                        </a>
                        
                        <a class="tag" href="/tags/ai" title="AI">
                            AI
                        </a>
                        
                    </div>
                    <h1>Learning Notes: Fine-Tuning Transformer Models</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        
                            Posted by 
                            
                                Jamie Zhang
                             
                            on 
                            Tuesday, September 2, 2025
                            
                            
                            
                            
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>




<article>
    <div class="container">
        <div class="row">

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                
                <h1 id="fine-tuning-transformer-models-a-beginners-guide">Fine-Tuning Transformer Models: A Beginner&rsquo;s Guide</h1>
<p>Fine-tuning is the process of adapting a pre-trained transformer model (like GPT, BERT, etc.) to a specific task or dataset. This blog will help you understand the basics, workflow, and best practices for fine-tuning.</p>
<h2 id="what-is-fine-tuning">What is Fine-Tuning?</h2>
<ul>
<li><strong>Pre-trained models</strong> learn general language patterns from massive datasets.</li>
<li><strong>Fine-tuning</strong> adapts these models to your specific task (e.g., sentiment analysis, Q&amp;A, summarization) using a smaller, task-specific dataset.</li>
</ul>

<div class="mermaid">
  
flowchart LR
    A[Pre-trained Model] --> B[Fine-tuning Process]
    C[Task-specific Dataset] --> B
    B --> D[Fine-tuned Model]

</div>
<script>
  function loadMermaid() {
    if (typeof mermaid === 'undefined') {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js';
      script.onload = function() {
        
        mermaid.initialize({
          startOnLoad: true,
          theme: 'default',
          flowchart: {
            useMaxWidth: true,
            htmlLabels: true
          }
        });
        
        
        const mermaidElements = document.querySelectorAll('.mermaid');
        if (mermaidElements.length > 0) {
          mermaid.init(mermaidElements);
        }
      };
      
      
      script.onerror = function(e) {
        console.error('Failed to load Mermaid library', e);
      };
      
      document.head.appendChild(script);
    } else {
      
      const mermaidElements = document.querySelectorAll('.mermaid');
      if (mermaidElements.length > 0) {
        mermaid.init(mermaidElements);
      }
    }
  }
  
  
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', loadMermaid);
  } else {
    
    window.setTimeout(loadMermaid, 0);
  }
</script>
<h2 id="why-fine-tune">Why Fine-Tune?</h2>
<ul>
<li>Saves time and resources (no need to train from scratch)</li>
<li>Leverages powerful language understanding</li>
<li>Achieves state-of-the-art results on custom tasks</li>
</ul>
<h2 id="typical-fine-tuning-workflow">Typical Fine-Tuning Workflow</h2>
<ol>
<li><strong>Choose a Pre-trained Model</strong>
<ul>
<li>Popular choices: GPT, BERT, RoBERTa, T5, etc.</li>
</ul>
</li>
<li><strong>Prepare Your Dataset</strong>
<ul>
<li>Format data for your task (classification, generation, etc.)</li>
<li>Clean and split into train/validation sets</li>
</ul>
</li>
<li><strong>Set Up Training Environment</strong>
<ul>
<li>Use frameworks like Hugging Face Transformers, PyTorch, TensorFlow</li>
</ul>
</li>
<li><strong>Configure Hyperparameters</strong>
<ul>
<li>Learning rate, batch size, epochs, etc.</li>
</ul>
</li>
<li><strong>Train the Model</strong>
<ul>
<li>Monitor loss and accuracy</li>
<li>Use early stopping to prevent overfitting</li>
</ul>
</li>
<li><strong>Evaluate &amp; Test</strong>
<ul>
<li>Check performance on validation/test data</li>
<li>Adjust and retrain if needed</li>
</ul>
</li>
<li><strong>Deploy</strong>
<ul>
<li>Integrate into your application or workflow</li>
</ul>
</li>
</ol>

<div class="mermaid">
  
flowchart TB
    A[Choose Model] --> B[Prepare Dataset]
    B --> C[Set Up Environment]
    C --> D[Configure Hyperparameters]
    D --> E[Train Model]
    E --> F[Evaluate & Test]
    F --> G[Deploy]
    G --> H{Satisfied with Results?}
    H -->|No| D
    H -->|Yes| I[Complete]

</div>
<script>
  function loadMermaid() {
    if (typeof mermaid === 'undefined') {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js';
      script.onload = function() {
        
        mermaid.initialize({
          startOnLoad: true,
          theme: 'default',
          flowchart: {
            useMaxWidth: true,
            htmlLabels: true
          }
        });
        
        
        const mermaidElements = document.querySelectorAll('.mermaid');
        if (mermaidElements.length > 0) {
          mermaid.init(mermaidElements);
        }
      };
      
      
      script.onerror = function(e) {
        console.error('Failed to load Mermaid library', e);
      };
      
      document.head.appendChild(script);
    } else {
      
      const mermaidElements = document.querySelectorAll('.mermaid');
      if (mermaidElements.length > 0) {
        mermaid.init(mermaidElements);
      }
    }
  }
  
  
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', loadMermaid);
  } else {
    
    window.setTimeout(loadMermaid, 0);
  }
</script>
<h2 id="example-fine-tuning-with-hugging-face-transformers">Example: Fine-Tuning with Hugging Face Transformers</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_name <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;bert-base-uncased&#34;</span>
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> AutoModelForSequenceClassification<span style="color:#ff79c6">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#ff79c6">=</span> AutoTokenizer<span style="color:#ff79c6">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># Prepare your dataset (as Hugging Face Dataset object)</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># ...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>training_args <span style="color:#ff79c6">=</span> TrainingArguments(
</span></span><span style="display:flex;"><span>    output_dir<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;./results&#34;</span>,
</span></span><span style="display:flex;"><span>    num_train_epochs<span style="color:#ff79c6">=</span><span style="color:#bd93f9">3</span>,
</span></span><span style="display:flex;"><span>    per_device_train_batch_size<span style="color:#ff79c6">=</span><span style="color:#bd93f9">8</span>,
</span></span><span style="display:flex;"><span>    evaluation_strategy<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;epoch&#34;</span>,
</span></span><span style="display:flex;"><span>    save_steps<span style="color:#ff79c6">=</span><span style="color:#bd93f9">10_000</span>,
</span></span><span style="display:flex;"><span>    save_total_limit<span style="color:#ff79c6">=</span><span style="color:#bd93f9">2</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>trainer <span style="color:#ff79c6">=</span> Trainer(
</span></span><span style="display:flex;"><span>    model<span style="color:#ff79c6">=</span>model,
</span></span><span style="display:flex;"><span>    args<span style="color:#ff79c6">=</span>training_args,
</span></span><span style="display:flex;"><span>    train_dataset<span style="color:#ff79c6">=</span>train_dataset,
</span></span><span style="display:flex;"><span>    eval_dataset<span style="color:#ff79c6">=</span>eval_dataset,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff79c6">.</span>train()
</span></span></code></pre></div><h2 id="parameter-efficient-fine-tuning-peft-techniques">Parameter-Efficient Fine-Tuning (PEFT) Techniques</h2>
<p>As transformer models grow larger, full fine-tuning becomes computationally expensive. Parameter-efficient fine-tuning (PEFT) methods address this by only training a small subset of parameters while freezing the rest of the model.</p>

<div class="mermaid">
  
graph LR
    A[Full Model] --> B{PEFT Method}
    B --> C[LoRA Matrices]
    B --> D[Adapter Layers]
    B --> E[Prefix Tokens]
    C --> F[Fine-tuned Model]
    D --> F
    E --> F

</div>
<script>
  function loadMermaid() {
    if (typeof mermaid === 'undefined') {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js';
      script.onload = function() {
        
        mermaid.initialize({
          startOnLoad: true,
          theme: 'default',
          flowchart: {
            useMaxWidth: true,
            htmlLabels: true
          }
        });
        
        
        const mermaidElements = document.querySelectorAll('.mermaid');
        if (mermaidElements.length > 0) {
          mermaid.init(mermaidElements);
        }
      };
      
      
      script.onerror = function(e) {
        console.error('Failed to load Mermaid library', e);
      };
      
      document.head.appendChild(script);
    } else {
      
      const mermaidElements = document.querySelectorAll('.mermaid');
      if (mermaidElements.length > 0) {
        mermaid.init(mermaidElements);
      }
    }
  }
  
  
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', loadMermaid);
  } else {
    
    window.setTimeout(loadMermaid, 0);
  }
</script>
<h3 id="lora-low-rank-adaptation">LoRA (Low-Rank Adaptation)</h3>
<p>LoRA is one of the most popular PEFT techniques. Instead of updating all model parameters during fine-tuning, LoRA injects trainable low-rank matrices into the model layers.</p>
<p><strong>How LoRA Works:</strong></p>
<ul>
<li>Freezes the pre-trained model weights</li>
<li>Adds trainable low-rank decomposition matrices to specific layers</li>
<li>Updates only these small matrices during training</li>
</ul>

<div class="mermaid">
  
flowchart LR
    subgraph "LoRA Mechanism"
        A[Original Weight Matrix W] --> B[Frozen]
        C[Low-Rank Matrices A,B] --> D[Trainable]
        B --> E[Updated W + AB]
        D --> E
    end

</div>
<script>
  function loadMermaid() {
    if (typeof mermaid === 'undefined') {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js';
      script.onload = function() {
        
        mermaid.initialize({
          startOnLoad: true,
          theme: 'default',
          flowchart: {
            useMaxWidth: true,
            htmlLabels: true
          }
        });
        
        
        const mermaidElements = document.querySelectorAll('.mermaid');
        if (mermaidElements.length > 0) {
          mermaid.init(mermaidElements);
        }
      };
      
      
      script.onerror = function(e) {
        console.error('Failed to load Mermaid library', e);
      };
      
      document.head.appendChild(script);
    } else {
      
      const mermaidElements = document.querySelectorAll('.mermaid');
      if (mermaidElements.length > 0) {
        mermaid.init(mermaidElements);
      }
    }
  }
  
  
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', loadMermaid);
  } else {
    
    window.setTimeout(loadMermaid, 0);
  }
</script>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> peft <span style="color:#ff79c6">import</span> LoraConfig, get_peft_model
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> AutoModelForSequenceClassification
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># Configure LoRA</span>
</span></span><span style="display:flex;"><span>lora_config <span style="color:#ff79c6">=</span> LoraConfig(
</span></span><span style="display:flex;"><span>    r<span style="color:#ff79c6">=</span><span style="color:#bd93f9">8</span>,  <span style="color:#6272a4"># Rank of the low-rank matrices</span>
</span></span><span style="display:flex;"><span>    lora_alpha<span style="color:#ff79c6">=</span><span style="color:#bd93f9">32</span>,
</span></span><span style="display:flex;"><span>    target_modules<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;query&#34;</span>, <span style="color:#f1fa8c">&#34;value&#34;</span>],  <span style="color:#6272a4"># Which modules to apply LoRA to</span>
</span></span><span style="display:flex;"><span>    lora_dropout<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.05</span>,
</span></span><span style="display:flex;"><span>    bias<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;none&#34;</span>,
</span></span><span style="display:flex;"><span>    modules_to_save<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;classifier&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># Apply LoRA to model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> AutoModelForSequenceClassification<span style="color:#ff79c6">.</span>from_pretrained(<span style="color:#f1fa8c">&#34;bert-base-uncased&#34;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> get_peft_model(model, lora_config)
</span></span></code></pre></div><p><strong>Benefits of LoRA:</strong></p>
<ul>
<li>Significantly reduces trainable parameters (often by 99%+)</li>
<li>Maintains model performance comparable to full fine-tuning</li>
<li>Enables efficient storage and switching between different fine-tuned versions</li>
</ul>
<h3 id="qlora-quantized-lora">QLoRA (Quantized LoRA)</h3>
<p>QLoRA builds upon LoRA by adding quantization techniques to further reduce memory requirements.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>4-bit NormalFloat quantization to compress the base model</li>
<li>Double quantization to further reduce memory footprint</li>
<li>Paged optimizers to handle memory spikes during training</li>
</ul>
<p>QLoRA allows fine-tuning models like 65B parameter LLaMA on consumer GPUs with as little as 48GB of VRAM.</p>

<div class="mermaid">
  
flowchart TB
    A[Full Precision Model] --> B[4-bit Quantization]
    B --> C[QLoRA Fine-tuning]
    D[LoRA Adapters] --> C
    C --> E[Quantized Fine-tuned Model]

</div>
<script>
  function loadMermaid() {
    if (typeof mermaid === 'undefined') {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js';
      script.onload = function() {
        
        mermaid.initialize({
          startOnLoad: true,
          theme: 'default',
          flowchart: {
            useMaxWidth: true,
            htmlLabels: true
          }
        });
        
        
        const mermaidElements = document.querySelectorAll('.mermaid');
        if (mermaidElements.length > 0) {
          mermaid.init(mermaidElements);
        }
      };
      
      
      script.onerror = function(e) {
        console.error('Failed to load Mermaid library', e);
      };
      
      document.head.appendChild(script);
    } else {
      
      const mermaidElements = document.querySelectorAll('.mermaid');
      if (mermaidElements.length > 0) {
        mermaid.init(mermaidElements);
      }
    }
  }
  
  
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', loadMermaid);
  } else {
    
    window.setTimeout(loadMermaid, 0);
  }
</script>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> peft <span style="color:#ff79c6">import</span> LoraConfig
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> BitsAndBytesConfig
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># Configure quantization</span>
</span></span><span style="display:flex;"><span>bnb_config <span style="color:#ff79c6">=</span> BitsAndBytesConfig(
</span></span><span style="display:flex;"><span>    load_in_4bit<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
</span></span><span style="display:flex;"><span>    bnb_4bit_use_double_quant<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
</span></span><span style="display:flex;"><span>    bnb_4bit_quant_type<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;nf4&#34;</span>,
</span></span><span style="display:flex;"><span>    bnb_4bit_compute_dtype<span style="color:#ff79c6">=</span>torch<span style="color:#ff79c6">.</span>bfloat16,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># Configure QLoRA</span>
</span></span><span style="display:flex;"><span>lora_config <span style="color:#ff79c6">=</span> LoraConfig(
</span></span><span style="display:flex;"><span>    r<span style="color:#ff79c6">=</span><span style="color:#bd93f9">64</span>,
</span></span><span style="display:flex;"><span>    lora_alpha<span style="color:#ff79c6">=</span><span style="color:#bd93f9">16</span>,
</span></span><span style="display:flex;"><span>    target_modules<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;q_proj&#34;</span>, <span style="color:#f1fa8c">&#34;v_proj&#34;</span>],
</span></span><span style="display:flex;"><span>    lora_dropout<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.1</span>,
</span></span><span style="display:flex;"><span>    bias<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;none&#34;</span>,
</span></span><span style="display:flex;"><span>    modules_to_save<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;classifier&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># Load quantized model with QLoRA</span>
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> AutoModelForSequenceClassification<span style="color:#ff79c6">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;llama-7b&#34;</span>,
</span></span><span style="display:flex;"><span>    quantization_config<span style="color:#ff79c6">=</span>bnb_config,
</span></span><span style="display:flex;"><span>    device_map<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;auto&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> get_peft_model(model, lora_config)
</span></span></code></pre></div><h3 id="other-peft-methods">Other PEFT Methods</h3>
<ol>
<li><strong>Adapter Tuning</strong>: Inserts small neural networks (adapters) between layers of the transformer</li>
<li><strong>Prefix Tuning</strong>: Adds trainable prefix tokens to each layer&rsquo;s input</li>
<li><strong>Prompt Tuning</strong>: Optimizes continuous prompt embeddings prepended to input text</li>
<li><strong>BitFit</strong>: Only trains the bias terms of the model</li>
</ol>

<div class="mermaid">
  
graph TD
    A[PEFT Methods] --> B[LoRA]
    A --> C[Adapter Tuning]
    A --> D[Prefix Tuning]
    A --> E[Prompt Tuning]
    A --> F[BitFit]
    
    B --> G[Low-rank matrices]
    C --> H[Small neural networks]
    D --> I[Prefix tokens]
    E --> J[Prompt embeddings]
    F --> K[Bias terms only]

</div>
<script>
  function loadMermaid() {
    if (typeof mermaid === 'undefined') {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js';
      script.onload = function() {
        
        mermaid.initialize({
          startOnLoad: true,
          theme: 'default',
          flowchart: {
            useMaxWidth: true,
            htmlLabels: true
          }
        });
        
        
        const mermaidElements = document.querySelectorAll('.mermaid');
        if (mermaidElements.length > 0) {
          mermaid.init(mermaidElements);
        }
      };
      
      
      script.onerror = function(e) {
        console.error('Failed to load Mermaid library', e);
      };
      
      document.head.appendChild(script);
    } else {
      
      const mermaidElements = document.querySelectorAll('.mermaid');
      if (mermaidElements.length > 0) {
        mermaid.init(mermaidElements);
      }
    }
  }
  
  
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', loadMermaid);
  } else {
    
    window.setTimeout(loadMermaid, 0);
  }
</script>
<h2 id="tips--best-practices">Tips &amp; Best Practices</h2>
<ul>
<li>Start with a small learning rate</li>
<li>Use data augmentation if your dataset is small</li>
<li>Monitor for overfitting</li>
<li>Use transfer learning for related tasks</li>
<li>For large models, consider parameter-efficient methods like LoRA</li>
<li>When memory is constrained, consider QLoRA for quantized models</li>
</ul>
<h2 id="common-pitfalls">Common Pitfalls</h2>
<ul>
<li><strong>Overfitting:</strong> Too many epochs or too small a dataset</li>
<li><strong>Data Leakage:</strong> Mixing train/test data</li>
<li><strong>Ignoring Validation:</strong> Always evaluate on unseen data</li>
<li><strong>Inefficient Resource Usage:</strong> Using full fine-tuning when PEFT methods would suffice</li>
<li><strong>Poor LoRA Configuration:</strong> Using inappropriate rank values or target modules</li>
</ul>
<h2 id="summary-table-pre-training-vs-fine-tuning">Summary Table: Pre-training vs. Fine-tuning</h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Pre-training</th>
<th>Fine-tuning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data Size</td>
<td>Massive (billions)</td>
<td>Small (thousands)</td>
</tr>
<tr>
<td>Purpose</td>
<td>General language</td>
<td>Task-specific</td>
</tr>
<tr>
<td>Compute Need</td>
<td>High</td>
<td>Moderate</td>
</tr>
<tr>
<td>Time</td>
<td>Weeks/months</td>
<td>Hours/days</td>
</tr>
</tbody>
</table>
<h2 id="summary-table-full-fine-tuning-vs-parameter-efficient-methods">Summary Table: Full Fine-tuning vs. Parameter-Efficient Methods</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Parameters Updated</th>
<th>Memory Usage</th>
<th>Training Speed</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>Full Fine-tuning</td>
<td>All parameters</td>
<td>High</td>
<td>Slow</td>
<td>High</td>
</tr>
<tr>
<td>LoRA</td>
<td>Low-rank matrices</td>
<td>Low</td>
<td>Fast</td>
<td>High</td>
</tr>
<tr>
<td>QLoRA</td>
<td>Low-rank matrices</td>
<td>Very Low</td>
<td>Fast</td>
<td>High</td>
</tr>
<tr>
<td>Adapter Tuning</td>
<td>Adapter layers</td>
<td>Low</td>
<td>Fast</td>
<td>Medium-High</td>
</tr>
</tbody>
</table>

<div class="mermaid">
  
graph TD
    A[Transformer Fine-tuning Approaches] --> B[Full Fine-tuning]
    A --> C[Parameter-Efficient Methods]
    C --> D[LoRA]
    C --> E[QLoRA]
    C --> F[Adapter Tuning]
    
    B --> |High Resource Usage| G[Best Performance]
    D --> |Balanced| H[Good Performance]
    E --> |Low Resource Usage| I[Good Performance]
    F --> |Low Resource Usage| J[Moderate Performance]

</div>
<script>
  function loadMermaid() {
    if (typeof mermaid === 'undefined') {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js';
      script.onload = function() {
        
        mermaid.initialize({
          startOnLoad: true,
          theme: 'default',
          flowchart: {
            useMaxWidth: true,
            htmlLabels: true
          }
        });
        
        
        const mermaidElements = document.querySelectorAll('.mermaid');
        if (mermaidElements.length > 0) {
          mermaid.init(mermaidElements);
        }
      };
      
      
      script.onerror = function(e) {
        console.error('Failed to load Mermaid library', e);
      };
      
      document.head.appendChild(script);
    } else {
      
      const mermaidElements = document.querySelectorAll('.mermaid');
      if (mermaidElements.length > 0) {
        mermaid.init(mermaidElements);
      }
    }
  }
  
  
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', loadMermaid);
  } else {
    
    window.setTimeout(loadMermaid, 0);
  }
</script>
<hr>
<h2 id="further-reading--resources">Further Reading &amp; Resources</h2>
<ul>
<li><a href="https://huggingface.co/docs/transformers/training">Hugging Face Transformers Docs</a></li>
<li><a href="https://ruder.io/nlp-transfer-learning/">Transfer Learning in NLP</a></li>
<li><a href="https://mccormickml.com/2019/07/22/BERT-fine-tuning/">Fine-Tuning BERT (Blog)</a></li>
<li><a href="https://arxiv.org/abs/2106.09685">LoRA Paper</a></li>
<li><a href="https://arxiv.org/abs/2305.14314">QLoRA Paper</a></li>
<li><a href="https://huggingface.co/docs/peft/">Hugging Face PEFT Documentation</a></li>
</ul>


                
                
<div class="entry-shang text-center">
    
	    <p>「真诚赞赏，手留余香」</p>
	
	<button class="zs show-zs btn btn-bred">赞赏支持</button>
</div>
<div class="zs-modal-bg"></div>
<div class="zs-modal-box">
	<div class="zs-modal-head">
		<button type="button" class="close">×</button>
		<span class="author"><a href="http://akjamie.github.io/"><img src="/img/favicon.png" />Jamie&#39;s Blog</a></span>
        
	        <p class="tip"><i></i><span>真诚赞赏，手留余香</span></p>
		
 
	</div>
	<div class="zs-modal-body">
		<div class="zs-modal-btns">
			<button class="btn btn-blink" data-num="2">2元</button>
			<button class="btn btn-blink" data-num="5">5元</button>
			<button class="btn btn-blink" data-num="10">10元</button>
			<button class="btn btn-blink" data-num="50">50元</button>
			<button class="btn btn-blink" data-num="100">100元</button>
			<button class="btn btn-blink" data-num="1">任意金额</button>
		</div>
		<div class="zs-modal-pay">
			<button class="btn btn-bred" id="pay-text">2元</button>
			<p>使用<span id="pay-type">微信</span>扫描二维码完成支付</p>
			<img src="/img/reward/wechat-2.png"  id="pay-image"/>
		</div>
	</div>
	<div class="zs-modal-footer">
		<label><input type="radio" name="zs-type" value="wechat" class="zs-type" checked="checked"><span ><span class="zs-wechat"><img src="/img/reward/wechat-btn.png"/></span></label>
		<label><input type="radio" name="zs-type" value="alipay" class="zs-type" class="zs-alipay"><img src="/img/reward/alipay-btn.png"/></span></label>
	</div>
</div>
<script type="text/javascript" src="/js/reward.js"></script>

                

                
                <hr>
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/post/2025-08-31-learning-notes-transformer/" data-toggle="tooltip" data-placement="top" title="Understanding Transformer Architecture: The Foundation of Modern AI">&larr;
                            Previous Post</a>
                    </li>
                    
                    
                </ul>
                

                




            </div>

            
            
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
            

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/aws" title="aws">
                            aws
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/docker" title="docker">
                            docker
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/gcp" title="gcp">
                            gcp
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/k8s" title="k8s">
                            k8s
                        </a>
                        
                        
                        
                        <a href="/tags/kubernetes" title="kubernetes">
                            kubernetes
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/llm" title="llm">
                            llm
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/mongo" title="mongo">
                            mongo
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/nosql" title="nosql">
                            nosql
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/python" title="python">
                            python
                        </a>
                        
                        
                        
                        <a href="/tags/redis" title="redis">
                            redis
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/spring" title="spring">
                            spring
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/spring-cloud" title="spring cloud">
                            spring cloud
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/transaction-management" title="transaction management">
                            transaction management
                        </a>
                        
                        
                        
                        <a href="/tags/transformer" title="transformer">
                            transformer
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/vault" title="vault">
                            vault
                        </a>
                        
                        
                        
                        
                        
                        
                    </div>
                </section>
                

                
                
                <section>
                    <hr>
                    <h5>FRIENDS</h5>
                    <ul class="list-inline">
                        
                    </ul>
                </section>
                
            </div>
        </div>
    </div>
</article>




<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    
                    <li>
                        <a href="mailto:akjamie.zhang@outlook.com">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		           
                    
                    
                    
                    

		            
                    
                    <li>
                        <a target="_blank" href="/img/wechat-qrcode.png">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-weixin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		            
                    
                    <li>
                        <a target="_blank" href="https://github.com/akjamie">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		            
                    
                    
                    
                    <li>
                        <a target="_blank" href="https://www.linkedin.com/in/jamie-zhang">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		           
                    
                    
                    
                    
                    
                    
            
            
            
           
                   <li>
                       <a href='' rel="alternate" type="application/rss+xml" title="Jamie&#39;s Blog" >
                           <span class="fa-stack fa-lg">
                               <i class="fas fa-circle fa-stack-2x"></i>
                               <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                           </span>
                       </a>
                   </li>
            
             </ul>
		<p class="copyright text-muted">
                    Copyright &copy; Jamie&#39;s Blog 2025
                    
                </p>
            </div>
        </div>
    </div>
</footer>




<script>
    function loadAsync(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>






<script>
    
    if($('#tag_cloud').length !== 0){
        loadAsync("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>


<script>
    loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>






<script type="text/javascript">
    function generateCatalog(selector) {

        
        
        
        
            _containerSelector = 'div.post-container'
        

        
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        
        $(selector).html('')

        
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow" title="' + t + '">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    


    loadAsync("\/js\/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>







<script>

if (document.querySelector('.mermaid')) {
  
  var script = document.createElement('script');
  script.src = 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js';
  script.onload = function() {
    
    mermaid.initialize({ 
      startOnLoad: true,
      theme: 'default'
    });
  };
  document.head.appendChild(script);
}
</script>

</body>
</html>
</body>
</html>
